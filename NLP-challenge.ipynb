{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1503fe30",
   "metadata": {},
   "source": [
    "<title>SQL for AI Projects</title>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11905e5c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**Natural Language Processing Challenge**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup the DuckDB database, get you familiar with this Google Colab setup and then we'll dive into the NLP challenge exercises for the SQL for AI Projects course!\n",
    "\n",
    "##s Challenge Exercises\n",
    "\n",
    "1. Clean webpage text data\n",
    "2. Investigate customer review text\n",
    "3. Implement A/B test framework\n",
    "\n",
    "## Database Setup\n",
    "\n",
    "First things first, let's load up our Python libraries and setup access to our database.\n",
    "\n",
    "Don't worry if you're not familiar with Python - we'll just need to run the very first cell to initialize our SQL instance and there will be clear instructions whenever there is some non-SQL components.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or you could simply hit the `Run all` button on the top of the notebook just below the menu toolbar.\n",
    "\n",
    "This cell below will help us download and connect to a DuckDB database object within this notebook's temporary environment.\n",
    "\n",
    "There will also be a few outputs in the same cell including the following:\n",
    "\n",
    "* An interactive entity relationship diagram for our database is also as an output from the following cell. This will help us visualize all of the database tables and their relevant primary and foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# These pip install commands are required for Google Colab notebook environment\n",
    "!pip install --upgrade --quiet duckdb==1.3.1\n",
    "!pip install --quiet duckdb-engine==0.17.0\n",
    "!pip install --quiet jupysql==0.11.1\n",
    "\n",
    "# Also need to setup Git LFS for large file dowloads\n",
    "# This helps us to download large files stored on GitHub\n",
    "!apt-get install git-lfs -y\n",
    "!git lfs install\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/real-world-data-and-AI-challenges-with-SQL-3813163.git data\n",
    "\n",
    "# Need to change directory into \"data\" to run download database object\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct!\n",
    "%cd ..\n",
    "\n",
    "# Time to import all our Python packages\n",
    "import duckdb\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the jupysql extension to enable us to run SQL code in code cells\n",
    "%load_ext sql\n",
    "\n",
    "# We can now set some basic Pandas settings for rendering SQL outputs\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "# This is a convenience function to print long strings into multiple lines\n",
    "# You'll see this in action later on in our tutorial!\n",
    "def wrap_print(text):\n",
    "    print(textwrap.fill(text, width=80))\n",
    "\n",
    "# This is some boilerplate code to help us format printed output with wrapping\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output pre {\n",
    "    white-space: pre-wrap;\n",
    "    word-break: break-word;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "# Connecting to DuckDB\n",
    "# ====================\n",
    "\n",
    "# Setup the SQL connection\n",
    "connection = duckdb.connect(\"data/data.db\")\n",
    "%sql connection\n",
    "\n",
    "# Run a few test queries using both connections\n",
    "tables = connection.execute(\"SHOW TABLES\").fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "preview_counts_list = []\n",
    "for table_name in table_names:\n",
    "    try:\n",
    "        preview_counts_list.append(\n",
    "            connection.execute(f\"\"\"\n",
    "                SELECT '{table_name}' AS table_name,\n",
    "                    COUNT(*) AS record_count\n",
    "                FROM {table_name}\"\"\").fetchdf()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not preview table {table_name}: {e}\")\n",
    "        \n",
    "\n",
    "print(\"‚úÖ Database is now ready!\")\n",
    "\n",
    "print(\"\\nüìã Show count of rows from each table in the database:\")\n",
    "\n",
    "# Combine all dataframes in preview_df_list\n",
    "preview_counts_df = pd.concat(preview_counts_list, ignore_index=True)\n",
    "\n",
    "display(preview_counts_df)\n",
    "\n",
    "display(HTML('''\n",
    "<iframe width=\"100%\" height=\"600\" src='https://dbdiagram.io/e/685279b3f039ec6d36c0c7e9/68527d19f039ec6d36c1813e'> </iframe>\n",
    "'''\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fac061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is commented out - but it can be used to run this notebook locally!\n",
    "# In fact - I used this while developing this notebook :)\n",
    "\n",
    "# import duckdb\n",
    "# import textwrap\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from IPython.display import HTML, display\n",
    "# \n",
    "# connection = duckdb.connect(\"data.db\")\n",
    "# %sql connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d40c1",
   "metadata": {},
   "source": [
    "# How to Run SQL Queries\n",
    "\n",
    "Let's quickly see how we can run SQL code in our Jupyter notebook.\n",
    "\n",
    "In our Colab environment we can run single or multi-line queries. We can also easily save the output of SQL queries as a local Pandas DataFrame object and even run subsequent SQL queries which can interact with these same DataFrame objects.\n",
    "\n",
    "## Single Line SQL Query\n",
    "\n",
    "We can use our notebook magic `%sql` at the start of a notebook cell to run a single line of SQL to query our database.\n",
    "\n",
    "Let's take a look at the first 5 rows from the `locations` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25719458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28416b",
   "metadata": {},
   "source": [
    "## Multi-Line SQL Query\n",
    "\n",
    "We can also run multi-line SQL queries by using a different notebook magic `%%sql` where we now have 2 percentage signs.\n",
    "\n",
    "We'll apply a filter on our `location` dataset and return 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8cf549",
   "metadata": {},
   "source": [
    "## Saving SQL Outputs\n",
    "\n",
    "By using the `<<` operator, we can assign the result of a SQL query (returned as a Pandas DataFrame) to a named Python variable in the notebook‚Äôs scope.\n",
    "\n",
    "### Single Line Assignment\n",
    "\n",
    "We can specify the name of the output variable directly after the `%sql` or `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36119368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql single_magic_df << SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91656938",
   "metadata": {},
   "source": [
    "We can now reference the Python variable directly as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python notebook scope\n",
    "single_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc8169",
   "metadata": {},
   "source": [
    "We can also use this same variable as a table reference within a DuckDB `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM single_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fc9bb",
   "metadata": {},
   "source": [
    "### Multi-line Assignment\n",
    "\n",
    "This assignment using `<<` also works with the `%%sql` (multi-line) magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ba366",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql multi_magic_df <<\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad734fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "multi_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f452d",
   "metadata": {},
   "source": [
    "When referencing the Python variable within DuckDB, we can also use it inside a multi-line SQL query using the `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05992467",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM multi_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebe3cc",
   "metadata": {},
   "source": [
    "# 1. Clean Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80ab2e",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise #1 - we‚Äôll clean and prepare the `html_data` column from the locations table so it‚Äôs ready for NLP.\n",
    "\n",
    "Here is an overview of what we will cover in this tutorial:\n",
    "\n",
    "* Deep dive into using `REGEXP_REPLACE`\n",
    "* Remove HTML tags\n",
    "* Clean up newline, whitespace and `&` characters\n",
    "* Apply advanced find-and-replace using `REGEXP_REPLACE`\n",
    "* Maintain original document structure\n",
    "\n",
    "## 1.1 Inspect Raw Data\n",
    "\n",
    "### 1.1.2 Viewing Raw HTML\n",
    "\n",
    "Let‚Äôs start by looking at a single row ‚Äî specifically for Yosemite National Park ‚Äî to see what kind of cleaning is needed.\n",
    "\n",
    "We‚Äôll use the `.loc` method in Pandas to inspect the raw HTML. In this case, our expression below is how we would implement - ‚ÄúGet the value from the first row of the DataFrame, specifically from the html_data column.‚Äù\n",
    "\n",
    "```python\n",
    "yosemite_html_example_df.loc[0, \"html_data\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da35d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql yosemite_html_example_df << SELECT html_data FROM locations WHERE location_id = 1;\n",
    "\n",
    "# We'll save this variable for use in a later cell\n",
    "yosemite_raw_html_string = yosemite_html_example_df.loc[0, \"html_data\"]\n",
    "\n",
    "print(yosemite_raw_html_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499313",
   "metadata": {},
   "source": [
    "### 1.1.2 Inspect Rendered Data\n",
    "\n",
    "As we can see - there is a lot of cleaning that needs to be done with this!\n",
    "\n",
    "Let's take a look at how we can print out our HTML and see how it would render on an actual webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08188e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(yosemite_raw_html_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cbb776",
   "metadata": {},
   "source": [
    "## 1.2 Removing HTML Tags\n",
    "\n",
    "After inspecting the HTML code and our rendered data above - a simple solution comes to mind - potentially we can remove all the tags to only keep us the main text contents that we would see when we visit the actual web page generated by the HTML code.\n",
    "\n",
    "We can employ some regular expressions - also known as **regex** - and use the `REGEXP_REPLACE` SQL function to help us get this done.\n",
    "\n",
    "### 1.2.1 Introduction to `REGEXP_REPLACE`\n",
    "\n",
    "We'll get very familiar with the `REGEXP_REPLACE` function as we'll be using it throughout this tutorial.\n",
    "\n",
    "An example query we will use below is as follows to extract the text data for Yosemite - we'll also store this as a Python variable `yosemite_removed_tags_df` so we can access it again later:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  REGEXP_REPLACE(html_data, '<[^>]+>', '', 'g') AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 1;\n",
    "```\n",
    "\n",
    "Below is a simple breadown of the query components is included here:\n",
    "\n",
    "| Function         | Purpose                                                |\n",
    "| ---------------- | ------------------------------------------------------ |\n",
    "| `REGEXP_REPLACE` | Performs regex-based text replacement                  |\n",
    "| `html_data     ` | The column of string data that we want to adjust       |\n",
    "| `'<[^>]+>'`      | Matches any HTML tag like `<p>`, `<h2>`, `<ul>` etc    |\n",
    "| `[ ... ]`        | Defines a character class to match with inside the [ ] |\n",
    "| `''`             | Replaces matched text with nothing (i.e. deletes them) |\n",
    "| `'g'`            | Global flag ‚Äî apply to all matches, not just the first |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37722c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_removed_tags_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(html_data, '<[^>]+>', '', 'g') AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f56f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yosemite_removed_tags_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434dbc0",
   "metadata": {},
   "source": [
    "## 1.3 Removing Newline Characters\n",
    "\n",
    "It looks like there's a few newline `\\n` characters now appear in our transformed `html_data` string.\n",
    "\n",
    "We can deploy our `REGEXP_REPLACE` function again to make this work to trim our text outputs and remove those newlines from our already transformed `text_data` column.\n",
    "\n",
    "This time - notice how I'm using the `yosemite_removed_tags_df` as the target for my `SELECT` statement in my SQL query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3488ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_removed_tags_trimmed_df <<\n",
    "SELECT\n",
    "    REGEXP_REPLACE(text_data, '[\\n]', '', 'g') AS text_data\n",
    "FROM yosemite_removed_tags_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yosemite_removed_tags_trimmed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94423ac",
   "metadata": {},
   "source": [
    "### 1.3.1 Pretty Printing Long Strings\n",
    "\n",
    "We can't really see the entire string when we just display it like we have above - so I've implemented a neat printing function which we can use to see our string in a slightly nicer format.\n",
    "\n",
    "The only catch is that we'll need to use our `.loc` notation to extract the text data from our Pandas DataFrame Python variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(yosemite_removed_tags_trimmed_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd4445",
   "metadata": {},
   "source": [
    "### 1.3.2 Fixing Our Mistakes\n",
    "\n",
    "Wait a minute...it looks like we have a few more issues!\n",
    "\n",
    "Our `REGEXP_REPLACE` might have removed our additional newline characters but now it looks like we've squished a few of our words together in the raw text data.\n",
    "\n",
    "We can remove these by using our trusty `REGEXP_REPLACE` again - but this time we replace the empty string character with a single whitespace.\n",
    "\n",
    "Let's apply our changes on the same `yosemite_removed_tags_df` Pandas DataFrame we used for our previous SQL query - but we will assign our output to a new variable called `yosemite_removed_tags_and_newlines_df`\n",
    "\n",
    "**Note** - yes, I know the long variable names seem like a pain...but we have a popular saying \"code is usually read many more times than it's written\" so you can think of this as the equivalent of \"a stitch in time, saves nine\" sort of thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_removed_tags_and_newlines_df <<\n",
    "SELECT\n",
    "    # This time swap out the '' character for ' '\n",
    "    REGEXP_REPLACE(text_data, '[\\n]', ' ', 'g') AS text_data\n",
    "FROM yosemite_removed_tags_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(yosemite_removed_tags_and_newlines_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaf04d",
   "metadata": {},
   "source": [
    "## 1.4 Further Text Cleaning\n",
    "\n",
    "Now we've got more cleaning to do - maybe we'll want to get rid of those pesky little `&amp;` characters and replace them with a single `&` character.\n",
    "\n",
    "We've also got a few too many whitespace characters here in our text.\n",
    "\n",
    "Let's apply our changes one at a time - but at some point we will need to think about how we can combine all of these changes in one go from our source `locations` table instead of applying these transformations one at a time.\n",
    "\n",
    "Let's perform the following transformations:\n",
    "1. Replace `&amp;` with `&`\n",
    "2. Replace one or more whitespace character with a single whitespace\n",
    "\n",
    "### 1.4.1 `REGEXP_REPLACE` With Special Characters\n",
    "\n",
    "Sometimes when using `REGEXP_REPLACE` we need to be careful with special characters when we are searching for a specific pattern. Try playing around with the `'&amp;'` below and you'll begin to see what a I mean! If you want to use it with the character class definition square brackets - we'll need to use the backslash character `\\` to escape important characters.\n",
    "\n",
    "Note that these days - it's easy enough to ask an AI to assist with your regular expressions - but back in the old day's we needed to always look these up in Google or use specific Regex checking tools like [\"I Hate Regex\"](https://ihateregex.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1477935",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_removed_tags_newlines_ampersand_df <<\n",
    "SELECT\n",
    "    REGEXP_REPLACE(text_data, '&amp;', '&', 'g') AS text_data\n",
    "FROM yosemite_removed_tags_and_newlines_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7baf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(yosemite_removed_tags_newlines_ampersand_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9217c",
   "metadata": {},
   "source": [
    "### 1.4.2 Regular Expression Character Class\n",
    "\n",
    "We can also use our character classes `[ ]+` to let our `REGEXP_REPLACE` to find and replace 1 or more whitespace characters in a row with a single whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10422ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_removed_tags_newlines_ampersand_spaces_df <<\n",
    "SELECT\n",
    "    # We can use [ ]+ to represent 1 or more spaces\n",
    "    REGEXP_REPLACE(text_data, '[ ]+', ' ', 'g') AS text_data\n",
    "FROM yosemite_removed_tags_newlines_ampersand_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(yosemite_removed_tags_newlines_ampersand_spaces_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa527a7a",
   "metadata": {},
   "source": [
    "## 1.5 Combining Transformations\n",
    "\n",
    "So let's say we want to apply all of our changes that we've identified so far in one-shot from the raw `locations` table within our database.\n",
    "\n",
    "We have the following transformations to apply:\n",
    "\n",
    "1. Remove HTML tags\n",
    "2. Replace multiple newline characters with a single space\n",
    "3. Replace funny ampersand `&amp;` characters\n",
    "\n",
    "### 1.5.1 Nested `REGEXP_REPLACE`\n",
    "\n",
    "We can again complete this task using our trust `REGEXP_REPLACE` function!\n",
    "\n",
    "The only catch here is that we'll need to use our function in a \"nested\" form to apply these changes one after another - and we'll need to think about the order of how we apply our changes.\n",
    "\n",
    "One approach I use to better understand the \"nesting\" behaviour is to always work from **inside-out** - the SQL engine will start from the most nested transformation first before applying the outer nested function.\n",
    "\n",
    "Let's give this a shot below and store our results into a variable called `yosemite_one_shot_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17007010",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql yosemite_one_shot_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "        # 1. Most inner function for tag cleanup\n",
    "        REGEXP_REPLACE(html_data, '<[^>]+>', '', 'g'),\n",
    "        # 2. Now we can clean up newline and all whitespace in one-shot\n",
    "        # We also remove any \\r returns and \\t tab characters\n",
    "        '[\\n\\r\\t\\ ]+', ' ', 'g'\n",
    "    ),\n",
    "    # 3. Now we can apply our & update\n",
    "    '&amp;', '&', 'g'\n",
    "  )\n",
    "   AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(yosemite_one_shot_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ae878",
   "metadata": {},
   "source": [
    "## 1.6 Testing Another Example\n",
    "\n",
    "We've been performing all our transformations so far on the Yosemite location data - let's take a look at another specific example to challenge our SQL skills and clean our data further!\n",
    "\n",
    "`location_id = 46` contains the LACMA Los Angeles County Museum of Art details.\n",
    "\n",
    "This will be a good example for us to implement even further data cleansing steps.\n",
    "\n",
    "Let's firstly print out our example record to see what we're playing with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76563748",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql museum_html_example_df << SELECT html_data FROM locations WHERE location_id = 46;\n",
    "\n",
    "# We'll save this variable for use in a later cell\n",
    "museum_raw_html_string = museum_html_example_df.loc[0, \"html_data\"]\n",
    "\n",
    "print(museum_raw_html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4241f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(museum_raw_html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(museum_raw_html_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eabebc5",
   "metadata": {},
   "source": [
    "### 1.6.1 Removing Brackets\n",
    "\n",
    "One of the first things I've noticed here is that we'll likely end up with the round brackets or paranethesis around the `(Los Angeles County Museum of Art)`\n",
    "\n",
    "We can aim to try and remove these brackets and also apply the same exact transformations we've seen with our Yosemite example.\n",
    "\n",
    "Let's try this first to see if we need to apply further transforms.\n",
    "\n",
    "We can use our trusty `REGEXP_REPLACE` to remove both the left and right paranthesis characters - however we'll need to be careful with how we apply the backslash `\\` to escape these special regular expression chraracters in our function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql museum_transformed_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "        # 1. Most inner function for tag cleanup\n",
    "        REGEXP_REPLACE(html_data, '<[^>]+>', '', 'g'),\n",
    "        # 2. Now we can clean up newline and all whitespace in one-shot\n",
    "        # Here we can also add in our \\( and \\) escaped paranetheses characters\n",
    "        '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "    ),\n",
    "    # 3. Now we can apply our & update\n",
    "    '&amp;', '&', 'g'\n",
    "  )\n",
    "   AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 46;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59965550",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(museum_transformed_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f99ef",
   "metadata": {},
   "source": [
    "### 1.6.1 Removing Specific Tags\n",
    "\n",
    "This is ALMOST there - but we have one more complication that we'll need to fix up!\n",
    "\n",
    "The very first line seems to have a repeat in the location name - we can see `LACMA Los Angeles County Museum of Art LACMA Los Angeles County Museum of Art` in our first line of the previous output.\n",
    "\n",
    "We'll need to inspect our raw HTML to see where this comes from - if we inspect the first few rows from our raw HTML that we had previously - we can see both a `title` and level 1 heading `h1` tag that repeats the location name.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<title>LACMA (Los Angeles County Museum of Art)</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>LACMA (Los Angeles County Museum of Art)</h1>\n",
    "...\n",
    "```\n",
    "\n",
    "### 1.6.2 Common Ordering Mistakes\n",
    "\n",
    "We should be right to also use our `REGEXP_REPLACE` in a nested fashion to remove this additional `title` tag to remove the duplicate in our transformed text output - however we'll need to be careful with the order of which we apply the `REGEXP_REPLACE` transformations.\n",
    "\n",
    "If we were to apply this title tag removal **after** we've already removed **all** of our tags - then nothing would happen!\n",
    "\n",
    "Let's demonstrate this in action before we see how we should fix it - imagine we wanted to apply `<title>` tag removal after all of our original transformations using this regular expression: `(?si)<title>.*?</title>`\n",
    "\n",
    "The breakdown of what's happening in this regular expression is below:\n",
    "\n",
    "* (?si) ‚Äì two inline flags\n",
    "    + s: single‚Äëline mode, so the dot . matches everything, including newlines\n",
    "    + i: case‚Äëinsensitive, so it will match <title>, <Title>, <TITLE>, etc. \n",
    "* <title> ‚Äì literally matches the opening tag that we're after\n",
    "* `.*?` ‚Äì a non‚Äëgreedy match of any characters from just after <title> to the earliest possible </title>\n",
    "* </title> ‚Äì literally matches the closing `</title> tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql museum_transformed_removed_title_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        # 1. Most inner function for tag cleanup\n",
    "        REGEXP_REPLACE(html_data, '<[^>]+>', '', 'g'),\n",
    "        # 2. Now we can clean up newline and all whitespace in one-shot\n",
    "        # Here we can also add in our \\( and \\) escaped paranetheses characters\n",
    "        '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "      ),\n",
    "      # 3. Now we can apply our & update\n",
    "      '&amp;', '&', 'g'\n",
    "    ),\n",
    "    # 4. Let's say we put in our <title> removal here...\n",
    "    '(?si)<title>.*?</title>', '', 'g'\n",
    "  ) AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 46;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2989560",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(museum_transformed_removed_title_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954cfa8",
   "metadata": {},
   "source": [
    "### 1.6.2 Fixing Up The Order\n",
    "\n",
    "We can still see the repetition right at the beginning of our text!\n",
    "\n",
    "This is because when we apply the `REGEXP_REPLACE` to remove our HTML tags - we inadvertantly also remove the `<title>` tags we are looking to replace so our follow-up `REGEXP_REPLACE` call doesn't see the data.\n",
    "\n",
    "Let's try this again - but we'll adjust the order of our replacements a little and store our outputs in `museum_transformed_removed_title_adjusted_order_df` (I know...the names are getting a bit long right?!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql museum_transformed_removed_title_adjusted_order_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        # 1. Let's say we put in our <title> removal here this time!\n",
    "        REGEXP_REPLACE(html_data, '(?si)<title>.*?</title>', '', 'g'),\n",
    "        # 2. Now we can remove all of our tags\n",
    "        '<[^>]+>', '', 'g'\n",
    "      ),\n",
    "      # 3. Now we can clean up newline and all whitespace in one-shot\n",
    "      # Here we can also add in our \\( and \\) escaped paranetheses characters\n",
    "      '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "    ),\n",
    "    # 4. Finally we can remove the ampersand and we're done!\n",
    "    '&amp;', '&', 'g'\n",
    "  ) AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 46;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(museum_transformed_removed_title_adjusted_order_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2e8af",
   "metadata": {},
   "source": [
    "## 1.7 Retaining Document Structure\n",
    "\n",
    "Excellent - we've managed to remove the repetition at the beginning of our museum example!\n",
    "\n",
    "But there's more we can do!\n",
    "\n",
    "For traditional NLP - this is probably good enough, we have extracted the raw text and cleaned up most of our tags, additional spaces and fixed the ampersand web-escaped characters.\n",
    "\n",
    "However - for modern LLMs we can take it further and attempt to retain as much of our original document structure as possible. We can do this by further manipulating our raw text into discrete sections.\n",
    "\n",
    "For this exercise - we will need to really inspect our raw HTML to see how we might apply a good generalized rule and apply it across all our documents.\n",
    "\n",
    "### 1.7.1 Identifying What to Retain\n",
    "\n",
    "If we dive into our raw HTML - we'll be able to see how our level 2 headings might be useful to structure our cleaned text output.\n",
    "\n",
    "These `<h2> ... </h2>` tags can be used with our `REGEXP_REPLACE` to help retain the structure of the document.\n",
    "\n",
    "```html\n",
    "<h2>Summary</h2>\n",
    "<p>The largest art museum in the western United States, with a collection of nearly 150,000 works spanning the history of art from ancient times to the present.</p>\n",
    "</section>\n",
    "<section id=\"best-time\">\n",
    "<h2>Best Time to Visit</h2>\n",
    "<p>Spring and fall usually offer mild weather and smaller crowds. Always check local conditions, as climate can vary by elevation.</p>\n",
    "</section>\n",
    "```\n",
    "\n",
    "### 1.7.2 Advanced Find and Replace\n",
    "\n",
    "For our exercise - let's surround whatever contents are inside the H2 heading with a single pipe character. \n",
    "\n",
    "It's a good idea to surround the level 2 contents with a pipe characters `|` before and after the heading text.\n",
    "\n",
    "We can accomplish this find and replace task using the same `REGEXP_REPLACE` function but this time with a slightly different variation using variable substitution!\n",
    "\n",
    "We can use the regular expression: `'<h2>|</h2>'` to replace any occurences of `<h2>` or `</h2>` within the `REGEXP_REPLACE` command.\n",
    "\n",
    "```sql\n",
    "REGEXP_REPLACE(html_data, '(?si)<h2>(.*?)</h2>', '| \\1 |', 'g')\n",
    "```\n",
    "\n",
    "Here is a simple breakdown of this regular expression function:\n",
    "\n",
    "\n",
    "| Component         | Purpose                                                      |\n",
    "| ----------------- | ------------------------------------------------------------ |\n",
    "| `(?si)`           | Matches across multiple lines and is case-insensitive        |\n",
    "| `<h2>`            | Matches the literal opening tag `<h2>`                       |\n",
    "| `(.*?)`           | Capturing group that contains any character between the tags |\n",
    "| `</h2>`           | Matches the literal closing of the tag `</h2>`               |\n",
    "| `'| \\1 |'`        | A backreference to the first (and only) capturing group      |\n",
    "| `'g'`             |  Global flag ‚Äî apply to all matches, not just the first      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7400647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql museum_further_transformed_df <<\n",
    "SELECT\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        # 1. remove title tag contents\n",
    "        REGEXP_REPLACE(html_data, '(?si)<title>.*?</title>', '', 'g'),\n",
    "        # 2. surround level 2 contents with | characters\n",
    "        '(?si)<h2>(.*?)</h2>', '| \\1 |', 'g'\n",
    "      ),\n",
    "      # 3. clean up - remove all other tags\n",
    "      '<[^>]+>', '', 'g'\n",
    "    ),\n",
    "    # 4. further clean up of whitespace and newlines\n",
    "    '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "  ) AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 46;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a619e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(museum_further_transformed_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec714d46",
   "metadata": {},
   "source": [
    "### 1.7.3 Remove Arbitrary Text\n",
    "\n",
    "This is very close to perfect - but I've noticed one more step we can take to further clean up our output!\n",
    "\n",
    "If we look at the end of our text data output - we can see that our ending of the `text_data` field ends with the following:\n",
    "\n",
    "```text\n",
    "| Useful Links | View on Google Maps Wikipedia Article\n",
    "```\n",
    "\n",
    "It seems that our hyperlink information is removed due to our previous regular expression removing all of the HTML tags. \n",
    "\n",
    "We can apply another `REGEXP_REPLACE` at the end of our series of transformations to remove everything from `Useful Links` to the end of the string.\n",
    "\n",
    "I've also noticed an additional single whitespace character at the beginning of our text-string - so let's also apply a simple `TRIM` function to strip out all leading and trailing whitespace characters also.\n",
    "\n",
    "This will be our final transformation - so let's store our outputs as the variable `museum_final_transformed_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql museum_final_transformed_df <<\n",
    "SELECT\n",
    "  TRIM(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        REGEXP_REPLACE(\n",
    "          REGEXP_REPLACE(\n",
    "            # 1. remove title tag contents\n",
    "            REGEXP_REPLACE(html_data, '(?si)<title>.*?</title>', '', 'g'),\n",
    "            # 2. surround level 2 contents with | characters\n",
    "            '(?si)<h2>(.*?)</h2>', '| \\1 |', 'g'\n",
    "          ),\n",
    "          # 3. clean up - remove all other tags\n",
    "          '<[^>]+>', '', 'g'\n",
    "        ),\n",
    "        # 4. further clean up of whitespace and newlines\n",
    "        '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "      ),\n",
    "      # 5. remove the final useful links / wiki missing links\n",
    "      # We'll need to escape the pipe character as it's special!\n",
    "      # The $ denotes the end of the line so we remove everything from | Useful...\n",
    "      '\\| Useful Links.*$', '', 'g'\n",
    "    )\n",
    "  ) AS text_data\n",
    "FROM locations\n",
    "WHERE location_id = 46;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_print(museum_final_transformed_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ad93d",
   "metadata": {},
   "source": [
    "## 1.8 Apply Transformations to Entire Dataset\n",
    "\n",
    "This is perfect! Let's now remove our `WHERE` filter and apply this to our entire dataset and we're done for exercise 1!\n",
    "\n",
    "We can store our outputs as `locations_transformed_df` and we can quickly check that our transformations look alright for the previous records we were checking `location_id = 1` and `location_id = 46` for our Yosemite and Museum examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093404ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql locations_transformed_df <<\n",
    "SELECT\n",
    "  # we can keep all of our existing locations data here in our final dataset\n",
    "  locations.*,\n",
    "  TRIM(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        REGEXP_REPLACE(\n",
    "          REGEXP_REPLACE(\n",
    "            # 1. remove title tag contents\n",
    "            REGEXP_REPLACE(html_data, '(?si)<title>.*?</title>', '', 'g'),\n",
    "            # 2. surround level 2 contents with | characters\n",
    "            '(?si)<h2>(.*?)</h2>', '| \\1 |', 'g'\n",
    "          ),\n",
    "          # 3. clean up - remove all other tags\n",
    "          '<[^>]+>', '', 'g'\n",
    "        ),\n",
    "        # 4. further clean up of whitespace and newlines\n",
    "        '[\\n\\r\\t\\(\\) ]+', ' ', 'g'\n",
    "      ),\n",
    "      # 5. remove the final useful links / wiki missing links\n",
    "      # We'll need to escape the pipe character as it's special!\n",
    "      # The $ denotes the end of the line so we remove everything from | Useful...\n",
    "      '\\| Useful Links.*$', '', 'g'\n",
    "    )\n",
    "  ) AS text_data\n",
    "FROM locations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out our work!\n",
    "locations_transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Yosemite which is our first row\n",
    "# Pandas DataFrames are 0-indexed so the first record is the 0th row\n",
    "wrap_print(locations_transformed_df.loc[0, \"text_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Museum example which is our 46th row - index should be 45\n",
    "wrap_print(locations_transformed_df.loc[45, \"text_data\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
