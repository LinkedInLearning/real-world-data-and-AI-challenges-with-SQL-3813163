{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d70ce1dd",
   "metadata": {},
   "source": [
    "# SQL for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390f673",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Generative AI**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup the DuckDB database, get you familiar with this Google Colab setup and then we'll dive into the GenAI exercises for the SQL for AI Projects course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcfaad",
   "metadata": {},
   "source": [
    "### Practice Exercises\n",
    "\n",
    "1. Perform GenAI metrics analysis to assess AI agent performance\n",
    "2. Implement statistical adjustments for multi-variate A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1d6be",
   "metadata": {},
   "source": [
    "### Database Setup\n",
    "\n",
    "First things first, let's load up our Python libraries and setup access to our database.\n",
    "\n",
    "Don't worry if you're not familiar with Python - we'll just need to run the very first cell to initialize our SQL instance and there will be clear instructions whenever there is some non-SQL components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9179a",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or you could simply hit the `Run all` button on the top of the notebook just below the menu toolbar.\n",
    "\n",
    "This cell below will help us download and connect to a DuckDB database object within this notebook's temporary environment.\n",
    "\n",
    "There will also be a few outputs in the same cell including the following:\n",
    "\n",
    "* An interactive entity relationship diagram for our database is also as an output from the following cell. This will help us visualize all of the database tables and their relevant primary and foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51397b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# These pip install commands are required for Google Colab notebook environment\n",
    "!pip install --upgrade --quiet duckdb==1.3.1\n",
    "!pip install --quiet duckdb-engine==0.17.0\n",
    "!pip install --quiet jupysql==0.11.1\n",
    "\n",
    "# Also need to setup Git LFS for large file dowloads\n",
    "# This helps us to download large files stored on GitHub\n",
    "!apt-get install git-lfs -y\n",
    "!git lfs install\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/real-world-data-and-AI-challenges-with-SQL-3813163.git data\n",
    "\n",
    "# Need to change directory into \"data\" to run download database object\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct!\n",
    "%cd ..\n",
    "\n",
    "# Time to import all our Python packages\n",
    "import duckdb\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the jupysql extension to enable us to run SQL code in code cells\n",
    "%load_ext sql\n",
    "\n",
    "# We can now set some basic Pandas settings for rendering SQL outputs\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "# This is a convenience function to print long strings into multiple lines\n",
    "# You'll see this in action later on in our tutorial!\n",
    "def wrap_print(text):\n",
    "    print(textwrap.fill(text, width=80))\n",
    "\n",
    "# This is some boilerplate code to help us format printed output with wrapping\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output pre {\n",
    "    white-space: pre-wrap;\n",
    "    word-break: break-word;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "# Connecting to DuckDB\n",
    "# ====================\n",
    "\n",
    "# Setup the SQL connection\n",
    "connection = duckdb.connect(\"data/data.db\")\n",
    "%sql connection\n",
    "\n",
    "# Run a few test queries using both connections\n",
    "tables = connection.execute(\"SHOW TABLES\").fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "preview_counts_list = []\n",
    "for table_name in table_names:\n",
    "    try:\n",
    "        preview_counts_list.append(\n",
    "            connection.execute(f\"\"\"\n",
    "                SELECT '{table_name}' AS table_name,\n",
    "                    COUNT(*) AS record_count\n",
    "                FROM {table_name}\"\"\").fetchdf()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not preview table {table_name}: {e}\")\n",
    "        \n",
    "\n",
    "print(\"‚úÖ Database is now ready!\")\n",
    "\n",
    "print(\"\\nüìã Show count of rows from each table in the database:\")\n",
    "\n",
    "# Combine all dataframes in preview_df_list\n",
    "preview_counts_df = pd.concat(preview_counts_list, ignore_index=True)\n",
    "\n",
    "display(preview_counts_df)\n",
    "\n",
    "display(HTML('''\n",
    "<iframe width=\"100%\" height=\"600\" src='https://dbdiagram.io/e/685279b3f039ec6d36c0c7e9/68527d19f039ec6d36c1813e'> </iframe>\n",
    "'''\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48be0fb1",
   "metadata": {},
   "source": [
    "# How to Run SQL Queries\n",
    "\n",
    "Let's quickly see how we can run SQL code in our Jupyter notebook.\n",
    "\n",
    "In our Colab environment we can run single or multi-line queries. We can also easily save the output of SQL queries as a local Pandas DataFrame object and even run subsequent SQL queries which can interact with these same DataFrame objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7310beb",
   "metadata": {},
   "source": [
    "## Single Line SQL Query\n",
    "\n",
    "We can use our notebook magic `%sql` at the start of a notebook cell to run a single line of SQL to query our database.\n",
    "\n",
    "Let's take a look at the first 5 rows from the `locations` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7940064",
   "metadata": {},
   "source": [
    "## Multi-Line SQL Query\n",
    "\n",
    "We can also run multi-line SQL queries by using a different notebook magic `%%sql` where we now have 2 percentage signs.\n",
    "\n",
    "We'll apply a filter on our `location` dataset and return 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f482fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cee14",
   "metadata": {},
   "source": [
    "## Saving SQL Outputs\n",
    "\n",
    "By using the `<<` operator, we can assign the result of a SQL query (returned as a Pandas DataFrame) to a named Python variable in the notebook‚Äôs scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18597311",
   "metadata": {},
   "source": [
    "### Single Line Assignment\n",
    "\n",
    "We can specify the name of the output variable directly after the `%sql` or `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql single_magic_df << SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff68f",
   "metadata": {},
   "source": [
    "We can now reference the Python variable directly as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c158b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python notebook scope\n",
    "single_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7c3a4",
   "metadata": {},
   "source": [
    "We can also use this same variable as a table reference within a DuckDB `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM single_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8c9ff",
   "metadata": {},
   "source": [
    "### Multi-line Assignment\n",
    "\n",
    "This assignment using `<<` also works with the `%%sql` (multi-line) magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5c67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql multi_magic_df <<\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "multi_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b79baa",
   "metadata": {},
   "source": [
    "When referencing the Python variable within DuckDB, we can also use it inside a multi-line SQL query using the `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM multi_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687b1e3",
   "metadata": {},
   "source": [
    "# 1. GenAI Metrics\n",
    "\n",
    "In this section, we‚Äôre going to explore how our LLMs and AI agents are actually performing out in the wild ‚Äî using simulated interaction data from our Explore California use case.\n",
    "\n",
    "There are quite a few questions we‚Äôll answer, but don‚Äôt worry ‚Äî the SQL is straightforward, and each query helps us uncover something useful.\n",
    "\n",
    "---\n",
    "\n",
    "To make sense of all this, we‚Äôll organize our metrics into six key themes:\n",
    "\n",
    "- **üßÆ Usage & Token Metrics**  \n",
    "  How many tokens are being used? Which agents are the most verbose? This helps us understand efficiency and cost.\n",
    "\n",
    "- **‚ö° Latency & Reliability**  \n",
    "  Are agents responding quickly and reliably? We‚Äôll look at average and p95 latency, plus error and retry rates.\n",
    "\n",
    "- **üåü Feedback & Hallucinations**  \n",
    "  How do users feel about the responses? Are hallucinations dragging down ratings? We‚Äôll dig into the human feedback.\n",
    "\n",
    "- **üìö Retrieval & Context Behavior**  \n",
    "  If we‚Äôre using document retrieval (RAG), how much context is pulled in ‚Äî and does it help or hurt?\n",
    "\n",
    "- **üë• Session Patterns & Engagement**  \n",
    "  How do users interact over a session? Which visits are super active, and what does that tell us?\n",
    "\n",
    "- **üí∞ Cost & Efficiency**  \n",
    "  Finally, we‚Äôll estimate how much each model is costing us and how many tokens we‚Äôre getting per dollar.\n",
    "\n",
    "> Together, these give us a full 360¬∞ view of how our agents are doing ‚Äî from quality and responsiveness to economics and user trust.\n",
    "\n",
    "---\n",
    "\n",
    "Before we dive in, let‚Äôs take a quick peek at the `interactions` table ‚Äî the source of all our insights!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM interactions LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03251093",
   "metadata": {},
   "source": [
    "## 1.1 Token Usage\n",
    "\n",
    "These queries help us understand how much content each agent is generating (or using). Token usage is key when it comes to tracking cost and prompt efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 1. What is the average total tokens per agent?\n",
    "SELECT\n",
    "  agent_name,\n",
    "  AVG(prompt_tokens + completion_tokens) AS avg_total_tokens\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 2. What is the average completions per agent?\n",
    "SELECT\n",
    "  agent_name,\n",
    "  AVG(completion_tokens) AS avg_completion_tokens\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 3. Return the top 5 visits with the most total token usage\n",
    "SELECT\n",
    "  visit_id,\n",
    "  SUM(prompt_tokens + completion_tokens) AS total_tokens\n",
    "FROM interactions\n",
    "GROUP BY visit_id\n",
    "ORDER BY total_tokens DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b264cf91",
   "metadata": {},
   "source": [
    "## 1.2 Latency and Reliability\n",
    "\n",
    "Let‚Äôs look at latency and error rates to see how responsive and reliable each agent is. The 95th percentile latency helps us spot outliers that might frustrate users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 4. What is the average latency for each agent?\n",
    "SELECT\n",
    "  agent_name,\n",
    "  AVG(latency_ms) AS avg_latency\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91458cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 5. What is the 95th percentile for latency for all interactions?\n",
    "SELECT\n",
    "  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95_latency\n",
    "FROM interactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55430b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 6. What is the error rate split by agent?\n",
    "SELECT agent_name,\n",
    "       AVG(CASE WHEN error_flag THEN 1 ELSE 0 END) AS error_rate\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99ee76",
   "metadata": {},
   "source": [
    "## 1.3 Feedback & Hallucinations\n",
    "\n",
    "Now we‚Äôll dive into user feedback and see how it correlates with hallucinations and errors. This gives us a human lens on quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90484c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 7. Avg feedback rating per agent\n",
    "SELECT agent_name, AVG(feedback_rating) AS avg_feedback\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 8. Feedback vs hallucination\n",
    "SELECT hallucination_flag, AVG(feedback_rating) AS avg_feedback\n",
    "FROM interactions\n",
    "GROUP BY hallucination_flag;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de071340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 9. Count of low-feedback interactions (‚â§2) with errors\n",
    "SELECT COUNT(*) AS low_feedback_errors\n",
    "FROM interactions\n",
    "WHERE feedback_rating <= 2 AND error_flag = TRUE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 10. Top visits by retry count\n",
    "SELECT visit_id, SUM(retry_count) AS total_retries\n",
    "FROM interactions\n",
    "GROUP BY visit_id\n",
    "ORDER BY total_retries DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a994b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 11. Avg retries per agent\n",
    "SELECT agent_name, AVG(retry_count) AS avg_retries\n",
    "FROM interactions\n",
    "GROUP BY agent_name;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c074d77",
   "metadata": {},
   "source": [
    "## 1.4 Retrieval & Context Behavior\n",
    "\n",
    "If you‚Äôre using RAG (Retrieval-Augmented Generation), these metrics help you understand how many documents are being retrieved and whether more context leads to slower or more hallucinated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 12. Avg docs retrieved per agent\n",
    "SELECT agent_name, AVG(documents_retrieved) AS avg_docs\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 13. Context tokens vs latency\n",
    "SELECT context_tokens, AVG(latency_ms) AS avg_latency\n",
    "FROM interactions\n",
    "GROUP BY context_tokens\n",
    "ORDER BY context_tokens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 14. Hallucination rate by docs retrieved\n",
    "SELECT documents_retrieved,\n",
    "       AVG(CASE WHEN hallucination_flag THEN 1 ELSE 0 END) AS hallucination_rate\n",
    "FROM interactions\n",
    "GROUP BY documents_retrieved;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c255d7c",
   "metadata": {},
   "source": [
    "## 1.5 Session Patterns & Engagement\n",
    "\n",
    "How many interactions do users have per visit? Are there certain sessions with a lot of back-and-forth?\n",
    "\n",
    "We'll also dive into deeper metrics like how efficient completions are compared to prompts, how fast tokens are being generated, and how feedback levels vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd34f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 15. Avg interactions per visit\n",
    "SELECT AVG(interaction_count) FROM (\n",
    "  SELECT visit_id, COUNT(*) AS interaction_count\n",
    "  FROM interactions\n",
    "  GROUP BY visit_id\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3195b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "#  16. Top 10 most active visits\n",
    "SELECT visit_id, COUNT(*) AS interaction_count\n",
    "FROM interactions\n",
    "GROUP BY visit_id\n",
    "ORDER BY interaction_count DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b90a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "#  17. Hallucination rate by agent\n",
    "SELECT agent_name,\n",
    "       AVG(CASE WHEN hallucination_flag THEN 1 ELSE 0 END) AS hallucination_rate\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "#  18. Completion-to-prompt ratio\n",
    "SELECT agent_name,\n",
    "       AVG(CAST(completion_tokens AS FLOAT) / NULLIF(prompt_tokens, 0)) AS completion_to_prompt_ratio\n",
    "FROM interactions\n",
    "GROUP BY agent_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62597ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "#  19. Token throughput (tokens per ms)\n",
    "SELECT agent_name,\n",
    "       AVG((prompt_tokens + completion_tokens) * 1.0 / latency_ms) AS tokens_per_ms\n",
    "FROM interactions\n",
    "GROUP BY agent_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "#  20. Interaction count by feedback level\n",
    "SELECT feedback_rating, COUNT(*) AS count\n",
    "FROM interactions\n",
    "GROUP BY feedback_rating\n",
    "ORDER BY feedback_rating;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385d348",
   "metadata": {},
   "source": [
    "## 1.6 Cost & Efficiency\n",
    "\n",
    "Finally, let‚Äôs look at the cost of each model and agent. We‚Äôll also calculate how many tokens we‚Äôre getting per dollar ‚Äî a handy metric for cost-efficiency.\n",
    "\n",
    "Assuming that we have the following cost structure for our 3 different models used in Explore California:\n",
    "\n",
    "| Model  | Price per 1K Tokens (USD) |\n",
    "|--------|---------------------------|\n",
    "| Kimi   | $0.002                    |\n",
    "| GPT    | $0.030                    |\n",
    "| Gemini | $0.010                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 21. Total cost estimate\n",
    "SELECT\n",
    "  SUM((prompt_tokens + completion_tokens) / 1000.0 *\n",
    "      CASE model_name\n",
    "          WHEN 'kimi' THEN 0.002\n",
    "          WHEN 'gpt' THEN 0.03\n",
    "          WHEN 'gemini' THEN 0.01\n",
    "      END) AS total_cost_usd\n",
    "FROM interactions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 22. Cost breakdown by model\n",
    "SELECT model_name,\n",
    "  SUM((prompt_tokens + completion_tokens) / 1000.0 *\n",
    "      CASE model_name\n",
    "          WHEN 'kimi' THEN 0.002\n",
    "          WHEN 'gpt' THEN 0.03\n",
    "          WHEN 'gemini' THEN 0.01\n",
    "      END) AS cost_usd\n",
    "FROM interactions\n",
    "GROUP BY model_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e230992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 23. Cost per agent\n",
    "SELECT agent_name,\n",
    "  SUM((prompt_tokens + completion_tokens) / 1000.0 *\n",
    "      CASE model_name\n",
    "          WHEN 'kimi' THEN 0.002\n",
    "          WHEN 'gpt' THEN 0.03\n",
    "          WHEN 'gemini' THEN 0.01\n",
    "      END) AS cost_usd\n",
    "FROM interactions\n",
    "GROUP BY agent_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# 24. Tokens per dollar (efficiency)\n",
    "SELECT agent_name,\n",
    "       SUM(prompt_tokens + completion_tokens) AS total_tokens,\n",
    "       SUM((prompt_tokens + completion_tokens) / 1000.0 *\n",
    "           CASE model_name\n",
    "               WHEN 'kimi' THEN 0.002\n",
    "               WHEN 'gpt' THEN 0.03\n",
    "               WHEN 'gemini' THEN 0.01\n",
    "           END) AS cost_usd,\n",
    "       SUM(prompt_tokens + completion_tokens) /\n",
    "           SUM((prompt_tokens + completion_tokens) / 1000.0 *\n",
    "               CASE model_name\n",
    "                   WHEN 'kimi' THEN 0.002\n",
    "                   WHEN 'gpt' THEN 0.03\n",
    "                   WHEN 'gemini' THEN 0.01\n",
    "               END) AS tokens_per_dollar\n",
    "FROM interactions\n",
    "GROUP BY agent_name;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbe092",
   "metadata": {},
   "source": [
    "# 2. Experimentation Analysis\n",
    "\n",
    "Let's now shift gears to analyze our AI agents experiments.\n",
    "\n",
    "We'll first need to combine all of our `feature`, `visits` and `sales` data to make sure we are seeing a complete picture of our conversion journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69595f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    visits.visit_timestamp,\n",
    "    visits.visit_id,\n",
    "    visits.user_id,\n",
    "\n",
    "    # Flag which Agent was active for this visit or 'Control' otherwise\n",
    "    COALESCE(features.feature, 'Control') AS experiment_group,\n",
    "    \n",
    "\n",
    "    # Flag whether a sale occurred during this visit\n",
    "    CASE WHEN sales.sale_id IS NOT NULL THEN 1 ELSE 0 END AS sale_flag,\n",
    "\n",
    "    # Capture sale amount; default to 0 if no product linked\n",
    "    COALESCE(products.price_usd, 0) AS sale_amount\n",
    "\n",
    "  FROM visits\n",
    "  LEFT JOIN features \n",
    "    ON visits.visit_id = features.visit_id\n",
    "  LEFT JOIN sales \n",
    "    ON visits.visit_id = sales.visit_id\n",
    "  LEFT JOIN products \n",
    "    ON sales.product_id = products.product_id\n",
    "  WHERE visits.visit_timestamp BETWEEN DATE '2026-07-01' AND DATE '2026-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d03b7",
   "metadata": {},
   "source": [
    "## 2.1 A/B Test Framework\n",
    "\n",
    "We can run this A/B test just like we‚Äôve done before ‚Äî but since we‚Äôre testing 3 groups at the same time, we need to be more careful about false positives.\n",
    "\n",
    "To do that, we‚Äôll use Bonferroni‚Äôs adjustment, which lowers the p-value threshold to make sure our results are still statistically valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 1: Build base dataset with visit-level outcomes\n",
    "# ------------------------------------------------------\n",
    "WITH cte_base AS (\n",
    "  SELECT\n",
    "    visits.visit_timestamp,\n",
    "    visits.visit_id,\n",
    "    visits.user_id,\n",
    "    COALESCE(features.feature, 'Control') AS experiment_group,  # Assign agent or control group\n",
    "    CASE WHEN sales.sale_id IS NOT NULL THEN 1 ELSE 0 END AS sale_flag,  # Conversion flag\n",
    "    COALESCE(products.price_usd, 0) AS sale_amount  # Revenue per visit\n",
    "  FROM visits\n",
    "  LEFT JOIN features ON visits.visit_id = features.visit_id\n",
    "  LEFT JOIN sales ON visits.visit_id = sales.visit_id\n",
    "  LEFT JOIN products ON sales.product_id = products.product_id\n",
    "  WHERE visits.visit_timestamp BETWEEN DATE '2026-07-01' AND DATE '2026-12-31'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 2: Aggregate metrics for the control group\n",
    "# ------------------------------------------------------\n",
    "cte_control AS (\n",
    "  SELECT\n",
    "    COUNT(DISTINCT visit_id) AS control_visit_count,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END) AS control_sales_count,\n",
    "    SUM(sale_amount) AS control_sales_amount,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END)::FLOAT / COUNT(DISTINCT visit_id) AS control_conversion_rate\n",
    "  FROM cte_base\n",
    "  WHERE experiment_group = 'Control'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 3: Aggregate metrics for Agent A\n",
    "# ------------------------------------------------------\n",
    "cte_agent_a AS (\n",
    "  SELECT\n",
    "    COUNT(DISTINCT visit_id) AS agent_a_visit_count,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END) AS agent_a_sales_count,\n",
    "    SUM(sale_amount) AS agent_a_sales_amount,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END)::FLOAT / COUNT(DISTINCT visit_id) AS agent_a_conversion_rate\n",
    "  FROM cte_base\n",
    "  WHERE experiment_group = 'Agent A'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 4: Aggregate metrics for Agent B\n",
    "# ------------------------------------------------------\n",
    "cte_agent_b AS (\n",
    "  SELECT\n",
    "    COUNT(DISTINCT visit_id) AS agent_b_visit_count,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END) AS agent_b_sales_count,\n",
    "    SUM(sale_amount) AS agent_b_sales_amount,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END)::FLOAT / COUNT(DISTINCT visit_id) AS agent_b_conversion_rate\n",
    "  FROM cte_base\n",
    "  WHERE experiment_group = 'Agent B'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 5: Combine group-level stats into a single row\n",
    "# ------------------------------------------------------\n",
    "cte_combined AS (\n",
    "  SELECT *\n",
    "  FROM cte_control, cte_agent_a, cte_agent_b\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 6: Calculate uplifts, confidence intervals, and standard errors\n",
    "# ------------------------------------------------------\n",
    "cte_stats AS (\n",
    "  SELECT *,\n",
    "    \n",
    "    # Absolute uplift vs. control\n",
    "    agent_a_conversion_rate - control_conversion_rate AS agent_a_absolute_uplift,\n",
    "    agent_b_conversion_rate - control_conversion_rate AS agent_b_absolute_uplift,\n",
    "\n",
    "    # Bonferroni-adjusted confidence intervals (z = 2.241)\n",
    "    agent_a_conversion_rate - 2.241 * SQRT((agent_a_conversion_rate * (1 - agent_a_conversion_rate)) / agent_a_visit_count) AS agent_a_ci_lower,\n",
    "    agent_a_conversion_rate + 2.241 * SQRT((agent_a_conversion_rate * (1 - agent_a_conversion_rate)) / agent_a_visit_count) AS agent_a_ci_upper,\n",
    "\n",
    "    agent_b_conversion_rate - 2.241 * SQRT((agent_b_conversion_rate * (1 - agent_b_conversion_rate)) / agent_b_visit_count) AS agent_b_ci_lower,\n",
    "    agent_b_conversion_rate + 2.241 * SQRT((agent_b_conversion_rate * (1 - agent_b_conversion_rate)) / agent_b_visit_count) AS agent_b_ci_upper,\n",
    "\n",
    "    control_conversion_rate - 2.241 * SQRT((control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count) AS control_ci_lower,\n",
    "    control_conversion_rate + 2.241 * SQRT((control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count) AS control_ci_upper,\n",
    "    \n",
    "    # Standard error for uplift comparisons\n",
    "    SQRT(\n",
    "      (agent_a_conversion_rate * (1 - agent_a_conversion_rate)) / agent_a_visit_count +\n",
    "      (control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count\n",
    "    ) AS agent_a_control_se,\n",
    "\n",
    "    SQRT(\n",
    "      (agent_b_conversion_rate * (1 - agent_b_conversion_rate)) / agent_b_visit_count +\n",
    "      (control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count\n",
    "    ) AS agent_b_control_se\n",
    "  FROM cte_combined\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 7: Compute z-scores and apply Bonferroni significance threshold\n",
    "# ------------------------------------------------------\n",
    "cte_zscore AS (\n",
    "  SELECT *,\n",
    "    agent_a_absolute_uplift / agent_a_control_se AS agent_a_z_score,\n",
    "    agent_b_absolute_uplift / agent_b_control_se AS agent_b_z_score,\n",
    "\n",
    "    # Mark as significant if z exceeds Bonferroni-adjusted critical value\n",
    "    CASE WHEN agent_a_absolute_uplift / agent_a_control_se >= 2.241402727604947\n",
    "         THEN 'Significant' ELSE 'Not Significant' END AS agent_a_significance,\n",
    "\n",
    "    CASE WHEN agent_b_absolute_uplift / agent_b_control_se >= 2.241402727604947\n",
    "         THEN 'Significant' ELSE 'Not Significant' END AS agent_b_significance\n",
    "  FROM cte_stats\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 8: Final output with all summary metrics\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  # Agent A\n",
    "  agent_a_visit_count, agent_a_sales_count, agent_a_conversion_rate,\n",
    "  agent_a_absolute_uplift, agent_a_ci_lower, agent_a_ci_upper,\n",
    "  agent_a_z_score, agent_a_significance,\n",
    "\n",
    "  # Agent B\n",
    "  agent_b_visit_count, agent_b_sales_count, agent_b_conversion_rate,\n",
    "  agent_b_absolute_uplift, agent_b_ci_lower, agent_b_ci_upper,\n",
    "  agent_b_z_score, agent_b_significance,\n",
    "\n",
    "  # Control Group\n",
    "  control_visit_count, control_sales_count, control_conversion_rate,\n",
    "  control_ci_lower, control_ci_upper\n",
    "FROM cte_zscore;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bd375",
   "metadata": {},
   "source": [
    "## 2.2 Experimentation Insights\n",
    "\n",
    "Here is an example report we can generate using our calculated metrics from our A/B test framework.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Experiment Results Summary\n",
    "\n",
    "Our A/B test evaluated the performance of **Agent A** and **Agent B** compared to a **control group** to understand their impact on conversion rates.\n",
    "\n",
    "#### ‚úÖ Statistical Significance\n",
    "\n",
    "- **Agent A**\n",
    "  - **Z-score**: `44.19`\n",
    "  - **Result**: **Significant** at the 95% confidence level (Bonferroni-adjusted)\n",
    "\n",
    "- **Agent B**\n",
    "  - **Z-score**: `25.91`\n",
    "  - **Result**: **Significant** at the 95% confidence level (Bonferroni-adjusted)\n",
    "\n",
    "This indicates **strong evidence** that both agents outperformed the control group in conversion rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Conversion Performance\n",
    "\n",
    "| Metric                     | Control Group  | Agent A          | Agent B          |\n",
    "|---------------------------|----------------|------------------|------------------|\n",
    "| Number of Visits           | 25,334         | 57,320           | 18,674           |\n",
    "| Number of Conversions      | 968            | 6,769            | 1,941            |\n",
    "| Conversion Rate            | 3.82%          | 11.81%           | 10.39%           |\n",
    "| 95% CI (Conversion Rate)   | [3.55%, 4.09%] | [11.51%, 12.11%] | [9.89%, 10.89%]  |\n",
    "\n",
    "- **Agent A uplift over control**: **+7.99%**\n",
    "  - 95% CI for uplift: [7.31%, 8.29%]\n",
    "- **Agent B uplift over control**: **+6.57%**\n",
    "  - 95% CI for uplift: [6.07%, 7.07%]\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Conclusion\n",
    "\n",
    "Both Agent A and Agent B produced **statistically significant improvements** in conversion rates compared to the control group.  \n",
    "Agent A performed the strongest, with nearly **8 percentage points of uplift**, suggesting it‚Äôs the most promising option for deployment to drive higher conversions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3fffd4",
   "metadata": {},
   "source": [
    "## 2.3 Analyzing Revenue Impact with ANOVA\n",
    "\n",
    "While conversion rate is a powerful metric for understanding user behavior, it's a **binary outcome** ‚Äî either a user converts or not.  \n",
    "Because of this, it's **not well-suited for ANOVA**, which assumes **continuous and normally distributed residuals**.\n",
    "\n",
    "Instead, we‚Äôll use **ANOVA** to compare the **`sale_amount`** (revenue per visit) across different model variants, giving us a way to test whether **any model produces significantly higher revenue**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Revenue-Based Comparison Across Agent Variants\n",
    "\n",
    "In this step, we‚Äôll analyze whether **different models** used within Agent A and Agent B groups result in different **average sale amounts**.\n",
    "\n",
    "This helps us identify which specific models not only convert users, but **generate higher-value transactions**.\n",
    "\n",
    "---\n",
    "\n",
    "### üßπ Preparing the Input Data\n",
    "\n",
    "To run ANOVA and follow-up pairwise comparisons (e.g. Tukey‚Äôs HSD), we need a dataset with:\n",
    "\n",
    "- One row per **visit**\n",
    "- A column for the **model variant** (e.g., `'Agent A - kimi'`, `'Agent B - gpt'`, `'Control'`)\n",
    "- A column for the **continuous outcome** (`sale_amount`)\n",
    "\n",
    "Example:\n",
    "\n",
    "| visit_id | experiment_group  | sale_amount |\n",
    "|----------|-------------------|-------------|\n",
    "| 001      | Agent A - kimi    | 199.00      |\n",
    "| 002      | Control           | 0.00        |\n",
    "| 003      | Agent B - gpt     | 329.00      |\n",
    "| ...      | ...               | ...         |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Example ANOVA Output\n",
    "\n",
    "| Source        | df  | Sum of Squares | Mean Square | F-Statistic | p-value |\n",
    "|---------------|-----|----------------|-------------|-------------|---------|\n",
    "| Between Groups|  4  | 122340.5       | 30585.1     | 8.42        | 0.00003 |\n",
    "| Within Groups | 1000| 3639180.2      | 3639.2      |             |         |\n",
    "| Total         | 1004| 3761520.7      |             |             |         |\n",
    "\n",
    "This tells us that **at least one group** differs significantly in average revenue.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Example Tukey's HSD Output\n",
    "\n",
    "| Group 1        | Group 2        | Mean Diff | p-adj  | Lower  | Upper  | Reject |\n",
    "|----------------|----------------|-----------|--------|--------|--------|--------|\n",
    "| Agent A - kimi | Control         | 12.5      | 0.001  | 5.4    | 19.6   | True   |\n",
    "| Agent B - gpt  | Agent A - kimi | -4.2      | 0.042  | -8.3   | -0.1   | True   |\n",
    "| ...            | ...             | ...       | ...    | ...    | ...    | ...    |\n",
    "\n",
    "The `Reject = True` column means we **can confidently say** the two groups have significantly different average sale amounts.\n",
    "\n",
    "---\n",
    "\n",
    "By using ANOVA + Tukey's HSD, we go beyond just asking *‚ÄúDoes it convert?‚Äù*  \n",
    "We start to answer: *‚ÄúWhich models drive the most valuable customers?‚Äù*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1bfe8c",
   "metadata": {},
   "source": [
    "## 2.5 SQL Implementation: Preparing Inputs for ANOVA & Tukey's HSD\n",
    "\n",
    "Before we can run our ANOVA and Tukey‚Äôs HSD tests, we need to generate a **visit-level dataset** that includes:\n",
    "\n",
    "- The **model name** used (from the `interactions` table)\n",
    "- The **experiment group** (e.g., Agent A, Agent B, or Control)\n",
    "- The **sale amount** (continuous revenue outcome per visit)\n",
    "\n",
    "We can generate this using a variation of our previous `cte_base` query by joining the `interactions` table and adding a composite label like:\n",
    "\n",
    "```sql\n",
    "COALESCE(features.feature || ' - ' || interactions.model_name, 'Control') AS experiment_group\n",
    "```\n",
    "\n",
    "This allows us to differentiate between individual model variants within each agent (e.g., `'Agent A - kimi'`, `'Agent B - gpt'`, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example Query Output (Truncated)\n",
    "\n",
    "| visit_id | experiment_group  | model_name | sale_amount |\n",
    "|----------|-------------------|------------|-------------|\n",
    "| v001     | Agent A - kimi    | kimi       | 289.00      |\n",
    "| v002     | Control           | NULL       | 0.00        |\n",
    "| v003     | Agent B - gpt     | gpt        | 412.00      |\n",
    "| ...      | ...               | ...        | ...         |\n",
    "\n",
    "---\n",
    "\n",
    "This dataset will serve as the **input into your Python-based statistical test**, where each row is a single observation.  \n",
    "We'll now move into Python to run the ANOVA and post-hoc pairwise tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql anova_inputs_df <<\n",
    "  SELECT\n",
    "    visits.visit_id,\n",
    "    COALESCE(features.feature || ' - ' || interactions.model_name, 'Control') AS experiment_group,\n",
    "    COALESCE(products.price_usd, 0) AS sale_amount\n",
    "  FROM visits\n",
    "  LEFT JOIN features ON visits.visit_id = features.visit_id\n",
    "  LEFT JOIN sales ON visits.visit_id = sales.visit_id\n",
    "  LEFT JOIN products ON sales.product_id = products.product_id\n",
    "  LEFT JOIN interactions on visits.visit_id = interactions.visit_id\n",
    "  WHERE visits.visit_timestamp BETWEEN DATE '2026-07-01' AND DATE '2026-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_inputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Fit ANOVA model\n",
    "anova_model = ols('sale_amount ~ C(experiment_group)', data=anova_inputs_df).fit()\n",
    "anova_results = sm.stats.anova_lm(anova_model, typ=2)\n",
    "print(anova_results)\n",
    "\n",
    "# Perform Tukey's HSD\n",
    "tukey_results = pairwise_tukeyhsd(\n",
    "    endog=anova_inputs_df['sale_amount'],\n",
    "    groups=anova_inputs_df['experiment_group'],\n",
    "    alpha=0.05\n",
    ")\n",
    "print(tukey_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c6d85",
   "metadata": {},
   "source": [
    "## 2.6 Experimental Analysis\n",
    "\n",
    "### ANOVA and Tukey's HSD Summary\n",
    "\n",
    "We conducted a one-way **ANOVA** to compare the mean `sale_amount` across multiple experiment groups. The results showed a **statistically significant difference** between at least one pair of groups:\n",
    "\n",
    "- **F-statistic**: `214.32`  \n",
    "- **p-value**: `< 1e-273`  \n",
    "- ‚úÖ **Conclusion**: Reject the null hypothesis ‚Äî not all groups have the same mean sale amount.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Post-hoc Comparison (Tukey's HSD)\n",
    "\n",
    "To identify **which specific groups differ**, we ran a Tukey‚Äôs HSD test. Here's what we found:\n",
    "\n",
    "#### üîπ No Significant Difference:\n",
    "- Among **Agent A** model variants (`gemini`, `gpt`, `kimi`) ‚Äî these models performed **similarly** in terms of sale amount.\n",
    "- Among **Agent B** variants ‚Äî also **no significant differences** between `gemini`, `gpt`, and `kimi`.\n",
    "\n",
    "#### üî∏ Significant Differences:\n",
    "- **All Agent A models significantly outperformed** their **Agent B counterparts**.\n",
    "- **All AI agents (A & B)** significantly outperformed the **Control** group (by over $148 per visit on average).\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Conclusion\n",
    "\n",
    "This analysis confirms that while different model variants **within the same agent group** perform similarly, the **agent group itself** (Agent A vs. Agent B vs. Control) plays a major role in driving higher revenue. Both Agent A and B lead to **statistically significant increases in sale amount**, with **Agent A performing the best overall**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
