{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf0d118",
   "metadata": {},
   "source": [
    "# SQL for AI Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659aadb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Supervised Machine Learning**\n",
    "\n",
    "In this Jupyter notebook - we'll quickly setup the DuckDB database, get you familiar with this Google Colab setup and then we'll dive into the supervised machine learning practice exercises for the SQL for AI Projects course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d65733",
   "metadata": {},
   "source": [
    "### Practice Exercises\n",
    "\n",
    "1. Create multi-class classification labeled dataset for product recommendation\n",
    "2. Create binary classification labeled dataset for purchase prediction\n",
    "3. Implement statistical power analysis and A/B test framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb504ca0",
   "metadata": {},
   "source": [
    "### Database Setup\n",
    "\n",
    "First things first, let's load up our Python libraries and setup access to our database.\n",
    "\n",
    "Don't worry if you're not familiar with Python - we'll just need to run the very first cell to initialize our SQL instance and there will be clear instructions whenever there is some non-SQL components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641a397",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To execute each cell in this notebook - you can click on the play button on the left of each cell or you could simply hit the `Run all` button on the top of the notebook just below the menu toolbar.\n",
    "\n",
    "This cell below will help us download and connect to a DuckDB database object within this notebook's temporary environment.\n",
    "\n",
    "There will also be a few outputs in the same cell including the following:\n",
    "\n",
    "* An interactive entity relationship diagram for our database is also as an output from the following cell. This will help us visualize all of the database tables and their relevant primary and foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e51b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup steps\n",
    "# ====================\n",
    "\n",
    "# These pip install commands are required for Google Colab notebook environment\n",
    "!pip install --upgrade --quiet duckdb==1.3.1\n",
    "!pip install --quiet duckdb-engine==0.17.0\n",
    "!pip install --quiet jupysql==0.11.1\n",
    "!pip install --quiet pyperclip==1.9.0\n",
    "\n",
    "# Also need to setup Git LFS for large file dowloads\n",
    "# This helps us to download large files stored on GitHub\n",
    "!apt-get install git-lfs -y\n",
    "!git lfs install\n",
    "\n",
    "# Clone GitHub repo into a \"data\" folder\n",
    "!git clone https://github.com/LinkedInLearning/real-world-data-and-AI-challenges-with-SQL-3813163.git data\n",
    "\n",
    "# Need to change directory into \"data\" to run download database object\n",
    "%cd data\n",
    "!git lfs pull\n",
    "\n",
    "# Then we need to change directory back up so all our paths are correct!\n",
    "%cd ..\n",
    "\n",
    "# Time to import all our Python packages\n",
    "import duckdb\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import pyperclip\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the jupysql extension to enable us to run SQL code in code cells\n",
    "%load_ext sql\n",
    "\n",
    "# We can now set some basic Pandas settings for rendering SQL outputs\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "# This is a convenience function to print long strings into multiple lines\n",
    "# You'll see this in action later on in our tutorial!\n",
    "def wrap_print(text):\n",
    "    print(textwrap.fill(text, width=80))\n",
    "\n",
    "# This is some boilerplate code to help us format printed output with wrapping\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output pre {\n",
    "    white-space: pre-wrap;\n",
    "    word-break: break-word;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "# Connecting to DuckDB\n",
    "# ====================\n",
    "\n",
    "# Setup the SQL connection\n",
    "connection = duckdb.connect(\"data/data.db\")\n",
    "%sql connection\n",
    "\n",
    "# Run a few test queries using both connections\n",
    "tables = connection.execute(\"SHOW TABLES\").fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "\n",
    "preview_counts_list = []\n",
    "for table_name in table_names:\n",
    "    try:\n",
    "        preview_counts_list.append(\n",
    "            connection.execute(f\"\"\"\n",
    "                SELECT '{table_name}' AS table_name,\n",
    "                    COUNT(*) AS record_count\n",
    "                FROM {table_name}\"\"\").fetchdf()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not preview table {table_name}: {e}\")\n",
    "        \n",
    "\n",
    "print(\"‚úÖ Database is now ready!\")\n",
    "\n",
    "print(\"\\nüìã Show count of rows from each table in the database:\")\n",
    "\n",
    "# Combine all dataframes in preview_df_list\n",
    "preview_counts_df = pd.concat(preview_counts_list, ignore_index=True)\n",
    "\n",
    "display(preview_counts_df)\n",
    "\n",
    "display(HTML('''\n",
    "<iframe width=\"100%\" height=\"600\" src='https://dbdiagram.io/e/685279b3f039ec6d36c0c7e9/68527d19f039ec6d36c1813e'> </iframe>\n",
    "'''\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a5086",
   "metadata": {},
   "source": [
    "# How to Run SQL Queries\n",
    "\n",
    "Let's quickly see how we can run SQL code in our Jupyter notebook.\n",
    "\n",
    "In our Colab environment we can run single or multi-line queries. We can also easily save the output of SQL queries as a local Pandas DataFrame object and even run subsequent SQL queries which can interact with these same DataFrame objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606b9af",
   "metadata": {},
   "source": [
    "## Single Line SQL Query\n",
    "\n",
    "We can use our notebook magic `%sql` at the start of a notebook cell to run a single line of SQL to query our database.\n",
    "\n",
    "Let's take a look at the first 5 rows from the `locations` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa82187",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d426dece",
   "metadata": {},
   "source": [
    "## Multi-Line SQL Query\n",
    "\n",
    "We can also run multi-line SQL queries by using a different notebook magic `%%sql` where we now have 2 percentage signs.\n",
    "\n",
    "We'll apply a filter on our `location` dataset and return 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d579358",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e4f16",
   "metadata": {},
   "source": [
    "## Saving SQL Outputs\n",
    "\n",
    "By using the `<<` operator, we can assign the result of a SQL query (returned as a Pandas DataFrame) to a named Python variable in the notebook‚Äôs scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9027fa92",
   "metadata": {},
   "source": [
    "### Single Line Assignment\n",
    "\n",
    "We can specify the name of the output variable directly after the `%sql` or `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql single_magic_df << SELECT * FROM locations LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43534b3e",
   "metadata": {},
   "source": [
    "We can now reference the Python variable directly as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python notebook scope\n",
    "single_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966934b",
   "metadata": {},
   "source": [
    "We can also use this same variable as a table reference within a DuckDB `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM single_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b4994",
   "metadata": {},
   "source": [
    "### Multi-line Assignment\n",
    "\n",
    "This assignment using `<<` also works with the `%%sql` (multi-line) magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql multi_magic_df <<\n",
    "SELECT\n",
    "  location_name,\n",
    "  description\n",
    "FROM locations\n",
    "WHERE location_id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe\n",
    "multi_magic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee3fd1",
   "metadata": {},
   "source": [
    "When referencing the Python variable within DuckDB, we can also use it inside a multi-line SQL query using the `%%sql` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM multi_magic_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a955d54",
   "metadata": {},
   "source": [
    "# 1. Multi-Class Classification\n",
    "\n",
    "Let's start our supervised machine learning challenge by exploring the data we'll be using for our product recommendation problem.\n",
    "\n",
    "Our goal is to generate a table with the following columns:\n",
    "\n",
    "* user_id\n",
    "* product_name\n",
    "* attributes\n",
    "\n",
    "However - we'll soon find out that we'll need to perform some extra data transformations to get our dataset ready for machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548cf24",
   "metadata": {},
   "source": [
    "## 1.1 Data Exploration\n",
    "\n",
    "Let's explore the following datasets we'll use to generate our labeled dataset:\n",
    "\n",
    "* sales\n",
    "* users\n",
    "* attributes\n",
    "* products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65344f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM sales LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM users LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b66fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM attributes LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM products LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4792d",
   "metadata": {},
   "source": [
    "## 1.2 Data Transformation\n",
    "\n",
    "Our next step is to create a **user-product interaction dataset** enriched with user behavior and preference attributes. It's designed to support downstream modeling tasks such as **product recommendation**, **user segmentation**, or **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177f186",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.1 Step-by-Step Breakdown\n",
    "\n",
    "1. **Join Users, Sales, and Products**  \n",
    "   Combines `users`, `sales`, and `products` tables to get a complete view of each user's purchase history ‚Äî mapping every user to the products they‚Äôve bought during the **2024‚Äì2025** timeframe.\n",
    "\n",
    "2. **Expand User Attributes**  \n",
    "   Uses `UNNEST(users.attributes)` to break out multi-valued attributes into individual rows. This allows us to later pivot each attribute as a binary indicator.\n",
    "\n",
    "3. **Join Attribute Descriptions**  \n",
    "   Links each `attribute_id` to its descriptive `attribute_name` by joining the `attributes` lookup table.\n",
    "\n",
    "4. **Pivot Attributes into One-Hot Columns**  \n",
    "   Performs a `PIVOT` to transform attribute rows into **one-hot encoded** columns. Each attribute becomes its own column, with a `1` if the user exhibits that behavior and `0` otherwise (using `COUNT(*)` as the aggregator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b8570",
   "metadata": {},
   "source": [
    "### 1.2.2 Final Output\n",
    "\n",
    "The result is a **wide-format dataset** where each row corresponds to a `(user_id, product_name)` pair, and columns represent the presence of various user traits ‚Äî such as:\n",
    "\n",
    "- `loves_hiking`\n",
    "- `budget_planner`\n",
    "- `frequent_subscriber`\n",
    "- `travels_with_family`\n",
    "- `young_professional`\n",
    "- *(and many more...)*\n",
    "\n",
    "This format is ideal for feeding into machine learning models that predict which products a user is likely to engage with ‚Äî based on their **past purchases** and **attribute profile**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331ffe7",
   "metadata": {},
   "source": [
    "### 1.2.3 Python Helper Function\n",
    "\n",
    "The code below generates a sorted list of unique attribute names from our `attributes` table and formats them as a string suitable for our following SQL `PIVOT` transformation ‚Äî with each value quoted and on its own line. It then copies the result to your clipboard using the `pyperclip` package, so you can easily paste it into a manual PIVOT statement in DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57651f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql unique_attributes_list << SELECT DISTINCT LOWER(attribute_name) AS attribute_name FROM attributes ORDER BY 1;\n",
    "\n",
    "# This creates the single quoted attribute names we'll use in our SQL query\n",
    "columns_list = \"(\\n'\" + \"',\\n'\".join(unique_attributes_list[\"attribute_name\"]) + \"'\\n)\"\n",
    "\n",
    "# We use this pyperclip package to copy the output directly to our clipboard\n",
    "# Then we can copy paste this directly into our SQL query below!\n",
    "pyperclip.copy(columns_list)\n",
    "\n",
    "# Print out first few records from our unique_attributes_list\n",
    "print(unique_attributes_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c5b20",
   "metadata": {},
   "source": [
    "## 1.3 SQL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql product_recommendation_df <<\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Join users, sales, and products to get purchase history\n",
    "# ------------------------------------------------------\n",
    "WITH cte_base AS (\n",
    "  SELECT\n",
    "    users.user_id,\n",
    "    products.product_name,\n",
    "\n",
    "    # Expand array of user attributes into individual rows\n",
    "    UNNEST(users.attributes) AS attribute_id\n",
    "\n",
    "  FROM users\n",
    "\n",
    "  # Join to sales to track which user bought which product\n",
    "  INNER JOIN sales\n",
    "    ON users.user_id = sales.user_id\n",
    "\n",
    "  # Join to product catalog to get product name\n",
    "  INNER JOIN products\n",
    "    ON sales.product_id = products.product_id\n",
    "\n",
    "  # Filter to purchases within the experiment window\n",
    "  WHERE sales.transaction_date BETWEEN DATE '2024-01-01' AND DATE '2025-12-31'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Join attributes table to get human-readable attribute names\n",
    "# ------------------------------------------------------\n",
    "cte_user_attributes AS (\n",
    "  SELECT\n",
    "    base.user_id,\n",
    "    base.product_name,\n",
    "\n",
    "    # Convert attribute names to lowercase for consistency\n",
    "    LOWER(attributes.attribute_name) AS attribute_name\n",
    "\n",
    "  FROM cte_base AS base\n",
    "\n",
    "  # Join to attribute dictionary table\n",
    "  INNER JOIN attributes\n",
    "    ON base.attribute_id = attributes.attribute_id\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Pivot user attributes into one-hot encoded columns\n",
    "# ------------------------------------------------------\n",
    "SELECT * FROM cte_user_attributes\n",
    "PIVOT (\n",
    "  # Each column indicates presence of an attribute for a user-product pair\n",
    "  COUNT(*)\n",
    "  FOR attribute_name IN (\n",
    "\n",
    "    # List of user attributes to pivot into individual columns\n",
    "    'big_box_store_visitor',\n",
    "    'bird_watcher',\n",
    "    'books_last_minute_trips',\n",
    "    'bookstore_browser',\n",
    "    'brand_comparison_shopper',\n",
    "    'brand_loyal_shopper',\n",
    "    'budget_planner',\n",
    "    'bulk_buyer',\n",
    "    'buy_now_pay_later_user',\n",
    "    'buys_weekend_deals',\n",
    "    'car_sharer',\n",
    "    'cashless_shopper',\n",
    "    'college_student',\n",
    "    'commutes_by_bike',\n",
    "    'commutes_by_car',\n",
    "    'commutes_by_transit',\n",
    "    'connects_to_public_wi_fi',\n",
    "    'convenience_focused',\n",
    "    'coupon_user',\n",
    "    'digital_nomad',\n",
    "    'discount_seeker',\n",
    "    'downloads_offline_maps',\n",
    "    'early_adopter',\n",
    "    'early_riser',\n",
    "    'eats_out_often',\n",
    "    'eco-conscious_consumer',\n",
    "    'eco_friendly',\n",
    "    'engages_with_newsletters',\n",
    "    'enjoys_quiet_places',\n",
    "    'family_with_kids',\n",
    "    'fitness_focused',\n",
    "    'frequent_coffee_buyer',\n",
    "    'frequent_pharmacy_visitor',\n",
    "    'frequent_returns',\n",
    "    'frequent_subscriber',\n",
    "    'frequent_takeout_customer',\n",
    "    'fuel_cost_sensitive',\n",
    "    'gift_card_giver',\n",
    "    'goes_solo',\n",
    "    'grocery_delivery_user',\n",
    "    'heavy_phone_user',\n",
    "    'high_end_grocery_buyer',\n",
    "    'history_buff',\n",
    "    'holiday_sale_shopper',\n",
    "    'home_cook',\n",
    "    'home_improvement_spender',\n",
    "    'impulse_buyer',\n",
    "    'international_tourist',\n",
    "    'last_minute_buyer',\n",
    "    'likes_camping',\n",
    "    'likes_luxury_trips',\n",
    "    'likes_museums',\n",
    "    'local_market_shopper',\n",
    "    'loves_hiking',\n",
    "    'loyalty_program_member',\n",
    "    'mobile_grocery_app_user',\n",
    "    'monthly_budget_adjuster',\n",
    "    'mountain_explorer',\n",
    "    'multichannel_shopper',\n",
    "    'nature_lover',\n",
    "    'night_owl',\n",
    "    'organic_food_buyer',\n",
    "    'owns_electric_vehicle',\n",
    "    'pays_with_mobile_apps',\n",
    "    'pet_friendly_traveler',\n",
    "    'photography_enthusiast',\n",
    "    'plans_trips_in_advance',\n",
    "    'posts_on_instagram',\n",
    "    'prefers_chain_stores',\n",
    "    'prefers_contactless',\n",
    "    'prefers_delivery_over_pickup',\n",
    "    'prefers_gift_experiences',\n",
    "    'prefers_local_brands',\n",
    "    'prefers_weekday_travel',\n",
    "    'price_comparer',\n",
    "    'rents_cars_for_travel',\n",
    "    'retired_traveler',\n",
    "    'rewards_points_user',\n",
    "    'road_trip_fan',\n",
    "    'rural_resident',\n",
    "    'shares_travel_on_social_media',\n",
    "    'shops_online_frequently',\n",
    "    'smart_home_user',\n",
    "    'solo_female_traveler',\n",
    "    'spiritual_traveler',\n",
    "    'stationery_and_supplies_shopper',\n",
    "    'streams_music_outdoors',\n",
    "    'subscribes_to_meal_kits',\n",
    "    'subscription_box_user',\n",
    "    'suburban_resident',\n",
    "    'sunset_seeker',\n",
    "    'takes_group_tours',\n",
    "    'takes_lots_of_selfies',\n",
    "    'tech_gadget_buyer',\n",
    "    'tracks_steps_on_phone',\n",
    "    'travel_blogger',\n",
    "    'travels_on_a_budget',\n",
    "    'travels_with_family',\n",
    "    'travels_with_partner',\n",
    "    'urban_resident',\n",
    "    'uses_google_maps',\n",
    "    'uses_smart_watch',\n",
    "    'uses_translation_apps',\n",
    "    'uses_travel_apps',\n",
    "    'walks_to_work',\n",
    "    'waterfall_chaser',\n",
    "    'weekly_grocery_shopper',\n",
    "    'wildlife_spotter',\n",
    "    'young_professional'\n",
    "  )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd63292",
   "metadata": {},
   "source": [
    "## 1.4 Inspecting Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a83ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM product_recommendation_df LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ddd02",
   "metadata": {},
   "source": [
    "# 2. Binary Classification\n",
    "\n",
    "We‚Äôre creating a dataset to predict whether a user will make a **purchase in the next 30 days**.\n",
    "\n",
    "This window can be adjusted ‚Äî shorter windows may lack signal, while longer ones reduce actionability.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Labeled Dataset and Time Splits\n",
    "\n",
    "To avoid data leakage, we use **out-of-time validation** where future time periods are held out for evaluation:\n",
    "\n",
    "- **Training**: 2024  \n",
    "- **Validation**: Jan‚ÄìJun 2025  \n",
    "- **Test**: Jul‚ÄìDec 2025\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Target Labeling Logic\n",
    "\n",
    "Here‚Äôs how we assign the binary target:\n",
    "\n",
    "- Users who purchased in 2024 ‚Üí `label = 1`, with a `label_date` randomly 1‚Äì30 days before the purchase.\n",
    "- All other users ‚Üí randomly assign a `label_date` between Jan 2024 and Dec 2025.\n",
    "- Look ahead 30 days from each `label_date`:\n",
    "  - If a purchase is found ‚Üí `label = 1`  \n",
    "  - Otherwise ‚Üí `label = 0`\n",
    "\n",
    "This simulates real prediction windows across all sets.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2.1 Edge Case Handling\n",
    "\n",
    "We **exclude** records where:\n",
    "\n",
    "- `label_date` doesn‚Äôt allow for a full 30-day lookahead in the same period\n",
    "- Examples: December 2024 (training), June 2025 (validation), December 2025 (test)\n",
    "\n",
    "This avoids future data leakage during training.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 SQL Logic Summary\n",
    "\n",
    "The SQL query builds this labeled dataset using the following steps:\n",
    "\n",
    "**Step-by-Step Summary of CTEs**\n",
    "\n",
    "- **`cte_sale_events`**  \n",
    "  Joins users with their sales and flags transactions in **2024** as part of the training period.\n",
    "\n",
    "- **`cte_train_positive_records`**  \n",
    "  For users with 2024 sales, assigns a `label_date` randomly up to 30 days before the purchase,  \n",
    "  with `label = 1` and `period = 'train'`.\n",
    "\n",
    "- **`cte_other_users`**  \n",
    "  Selects all users **not already labeled as training positives** and randomly assigns them a  \n",
    "  `label_date` between Jan 2024 and Dec 2025.\n",
    "\n",
    "- **`cte_other_records`**  \n",
    "  For each `label_date`, checks if a purchase occurs in the **next 30 days** to assign a  \n",
    "  `label` of `1` or `0`, and tags each record with a **train/validation/test** period based on the date.\n",
    "\n",
    "- **`cte_combined`**  \n",
    "  Combines:\n",
    "  - All **valid training positives** (excluding Dec 2024), and  \n",
    "  - All **other labeled records** with valid periods  \n",
    "  into a single dataset ready for modeling.\n",
    "\n",
    "> ‚ö†Ô∏è **Note on Randomness in DuckDB**  \n",
    "> The `RANDOM()` function in DuckDB is **not reproducible by default**, meaning it generates different values on each run.  \n",
    "> For consistent sampling (e.g. in experiments or production pipelines), consider exporting to Python and applying a fixed random seed there.\n",
    "\n",
    "---\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: DuckDB's `RANDOM()` function is **non-deterministic** ‚Äî results will change on each run.  \n",
    "> For reproducibility, sample label dates using Python with a fixed random seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql labeled_dataset_df <<\n",
    "# ------------------------------------------------------\n",
    "# 1. Join users with sales and flag whether it's part of the training window\n",
    "# ------------------------------------------------------\n",
    "WITH cte_sale_events AS (\n",
    "  SELECT\n",
    "    users.user_id,\n",
    "    sales.transaction_date,\n",
    "\n",
    "    # Flag sales in 2024 as eligible for training\n",
    "    CASE \n",
    "      WHEN transaction_date BETWEEN DATE '2024-01-01' AND DATE '2024-12-31' THEN 1\n",
    "      ELSE 0\n",
    "    END AS training_flag\n",
    "\n",
    "  FROM users\n",
    "  INNER JOIN sales ON users.user_id = sales.user_id\n",
    "\n",
    "  # Use 2024‚Äì2025 to allow for label windows that look 30 days into the future\n",
    "  WHERE sales.transaction_date BETWEEN DATE '2024-01-01' AND DATE '2025-12-31'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Create training positive examples (users who made a purchase in 2024)\n",
    "# ------------------------------------------------------\n",
    "cte_train_positive_records AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "\n",
    "    # Randomly assign a label_date up to 30 days before the sale\n",
    "    transaction_date - INTERVAL (CAST(1 + RANDOM() * 30 AS INTEGER)) DAYS AS label_date,\n",
    "\n",
    "    1 AS label,\n",
    "    'train' AS period\n",
    "  FROM cte_sale_events\n",
    "  WHERE training_flag = 1\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Select users with no training purchases and randomly assign label dates\n",
    "# ------------------------------------------------------\n",
    "cte_other_users AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "\n",
    "    # Assign random label dates between Jan 1 2024 and Dec 31 2025\n",
    "    DATE '2024-01-01' + INTERVAL (CAST(1 + RANDOM() * 730 AS INTEGER)) DAYS AS label_date\n",
    "  FROM users\n",
    "\n",
    "  # Exclude users who already have a labeled positive record\n",
    "  WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM cte_train_positive_records AS train\n",
    "    WHERE users.user_id = train.user_id\n",
    "  )\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. For the rest of the users, label them based on whether a sale happens within 30 days\n",
    "# ------------------------------------------------------\n",
    "cte_other_records AS (\n",
    "  SELECT\n",
    "    users.user_id,\n",
    "    users.label_date,\n",
    "\n",
    "    # Label = 1 if a sale occurs within 30 days after the label date\n",
    "    CASE \n",
    "      WHEN DATE_DIFF('DAY', users.label_date, sales.transaction_date) <= 30 THEN 1 \n",
    "      ELSE 0 \n",
    "    END AS label,\n",
    "\n",
    "    # Assign split period based on label_date\n",
    "    CASE\n",
    "      WHEN users.label_date BETWEEN DATE '2024-01-01' AND DATE '2024-11-30' THEN 'train'\n",
    "      WHEN users.label_date BETWEEN DATE '2025-01-01' AND DATE '2025-05-30' THEN 'validation'\n",
    "      WHEN users.label_date BETWEEN DATE '2025-07-01' AND DATE '2025-11-30' THEN 'test'\n",
    "      ELSE NULL\n",
    "    END AS period\n",
    "\n",
    "  FROM cte_other_users AS users\n",
    "  LEFT JOIN cte_sale_events AS sales\n",
    "    ON users.user_id = sales.user_id\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Combine positive and negative samples for training, validation, and test\n",
    "# ------------------------------------------------------\n",
    "cte_combined AS (\n",
    "  SELECT * \n",
    "  FROM cte_train_positive_records \n",
    "  WHERE label_date <= DATE '2024-11-30'   # Ensure train cutoff\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  SELECT * \n",
    "  FROM cte_other_records \n",
    "  WHERE period IS NOT NULL                # Only keep records with a defined period\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. Final labeled dataset for supervised learning\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  user_id,\n",
    "  label_date,\n",
    "  label,\n",
    "  period\n",
    "FROM cte_combined;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd607b4",
   "metadata": {},
   "source": [
    "### 2.3.1 Verify Training Labels\n",
    "\n",
    "Let's now analyze the binary target labels (`label = 1` or `0`) and verify how they are distributed across the different dataset splits:\n",
    "\n",
    "- **`train`**\n",
    "- **`validation`**\n",
    "- **`test`**\n",
    "\n",
    "For each split (`period`), we'll report the following metrics:\n",
    "\n",
    "- The total number of labeled records (`record_count`)\n",
    "- The proportion of positive labels (`positive_rate`), calculated as:  \n",
    "  `SUM(label) / COUNT(*)`\n",
    "\n",
    "This helps validate that the dataset is balanced appropriately across time-based partitions and that the label distribution is consistent and reasonable for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# ------------------------------------------------------\n",
    "# 1. Analyze label distribution across dataset splits\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  period,                            # Dataset split: 'train', 'validation', 'test'\n",
    "  \n",
    "  COUNT(*) AS record_count,          # Total number of labeled records in each period\n",
    "\n",
    "  # Positive label rate = proportion of label = 1\n",
    "  SUM(label) / COUNT(*) AS positive_rate\n",
    "\n",
    "FROM labeled_dataset_df\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Group by period to view distribution breakdown\n",
    "# ------------------------------------------------------\n",
    "GROUP BY period;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff8acf",
   "metadata": {},
   "source": [
    "## 2.4 Feature Engineering Overview\n",
    "\n",
    "To train a meaningful model, we combine two types of features:\n",
    "\n",
    "- **User attributes** (like preferences and behaviors)\n",
    "- **Recent activity**, such as how many times a user visited in the past 30 days\n",
    "\n",
    "Even though user attributes could change over time, we treat them as static for simplicity in this case study.\n",
    "\n",
    "### 2.4.1 SQL Implementation\n",
    "\n",
    "**Step-by-Step Breakdown of CTEs**\n",
    "\n",
    "1. `cte_user_attributes_base`\n",
    "- Joins each labeled record with user profile data.\n",
    "- Expands each user‚Äôs list of attribute IDs into individual rows using `UNNEST`.\n",
    "\n",
    "2. `cte_user_attributes`\n",
    "- Maps each attribute ID to a readable attribute name (e.g. `eco_friendly`, `coupon_user`).\n",
    "- Assigns a value of `1` to indicate the presence of each attribute (for one-hot encoding).\n",
    "\n",
    "3. `cte_user_visits`\n",
    "- Calculates the number of visits each user made **in the 30 days before their label date**.\n",
    "- Adds this as a numeric feature called `visits_last_30_days`.\n",
    "\n",
    "4. `cte_combined_features`\n",
    "- Combines the one-hot user attributes and the 30-day visit counts into a single table (long format).\n",
    "\n",
    "5. Final `SELECT` with `PIVOT`\n",
    "- Transforms the long-format table into wide-format:\n",
    "- Each unique `attribute_name` becomes its own column.\n",
    "- `MAX(attribute_value)` ensures correct numeric values are retained (e.g. visit counts or binary presence).\n",
    "\n",
    "---\n",
    "\n",
    "**Final Output**:  \n",
    "A machine learning‚Äìready dataset where each row represents a `(user_id, label_date)` pair, with:\n",
    "- One-hot encoded user attributes  \n",
    "- A numeric feature for recent visit activity  \n",
    "- The binary label (target) for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679302a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql labeled_binary_classification_dataset_df <<\n",
    "# ------------------------------------------------------\n",
    "# 1. Expand user attributes from labeled dataset\n",
    "# ------------------------------------------------------\n",
    "WITH cte_user_attributes_base AS (\n",
    "  SELECT\n",
    "    base.user_id,\n",
    "    base.label,\n",
    "    base.label_date,\n",
    "    base.period,\n",
    "    \n",
    "    # Unnest each user‚Äôs attributes into individual rows\n",
    "    UNNEST(users.attributes) AS attribute_id\n",
    "  FROM labeled_dataset_df AS base\n",
    "  INNER JOIN users ON base.user_id = users.user_id\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Map attribute IDs to readable attribute names (one-hot style)\n",
    "# ------------------------------------------------------\n",
    "cte_user_attributes AS (\n",
    "  SELECT\n",
    "    base.user_id,\n",
    "    base.label,\n",
    "    base.label_date,\n",
    "    base.period,\n",
    "    LOWER(attributes.attribute_name) AS attribute_name,\n",
    "    1 AS attribute_value  # Explicitly encode presence of attribute\n",
    "  FROM cte_user_attributes_base AS base\n",
    "  INNER JOIN attributes ON base.attribute_id = attributes.attribute_id\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Compute visit count feature within 30 days before label_date\n",
    "# ------------------------------------------------------\n",
    "cte_user_visits AS (\n",
    "  SELECT\n",
    "    base.user_id,\n",
    "    base.label,\n",
    "    base.label_date,\n",
    "    base.period,\n",
    "    'visits_last_30_days' AS attribute_name,\n",
    "\n",
    "    # Count visits within 30-day window before label_date\n",
    "    COALESCE(COUNT(visits.*), 0) AS attribute_value\n",
    "  FROM labeled_dataset_df AS base\n",
    "  LEFT JOIN visits\n",
    "    ON base.user_id = visits.user_id\n",
    "    AND visits.visit_timestamp BETWEEN (base.label_date - INTERVAL 31 DAYS) AND (base.label_date - INTERVAL 1 DAYS)\n",
    "  GROUP BY 1,2,3,4,5\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Union behavioral and demographic features\n",
    "# ------------------------------------------------------\n",
    "cte_combined_features AS (\n",
    "  SELECT * FROM cte_user_attributes\n",
    "  UNION ALL\n",
    "  SELECT * FROM cte_user_visits\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Pivot attributes into wide-format one-hot encoded dataset\n",
    "# ------------------------------------------------------\n",
    "SELECT *\n",
    "FROM cte_combined_features\n",
    "\n",
    "PIVOT (\n",
    "  # Use MAX to preserve numeric values (e.g. visit count) or presence (1)\n",
    "  MAX(attribute_value)\n",
    "  FOR attribute_name IN (\n",
    "    'visits_last_30_days',\n",
    "    'big_box_store_visitor',\n",
    "    'bird_watcher',\n",
    "    'books_last_minute_trips',\n",
    "    'bookstore_browser',\n",
    "    'brand_comparison_shopper',\n",
    "    'brand_loyal_shopper',\n",
    "    'budget_planner',\n",
    "    'bulk_buyer',\n",
    "    'buy_now_pay_later_user',\n",
    "    'buys_weekend_deals',\n",
    "    'car_sharer',\n",
    "    'cashless_shopper',\n",
    "    'college_student',\n",
    "    'commutes_by_bike',\n",
    "    'commutes_by_car',\n",
    "    'commutes_by_transit',\n",
    "    'connects_to_public_wi_fi',\n",
    "    'convenience_focused',\n",
    "    'coupon_user',\n",
    "    'digital_nomad',\n",
    "    'discount_seeker',\n",
    "    'downloads_offline_maps',\n",
    "    'early_adopter',\n",
    "    'early_riser',\n",
    "    'eats_out_often',\n",
    "    'eco-conscious_consumer',\n",
    "    'eco_friendly',\n",
    "    'engages_with_newsletters',\n",
    "    'enjoys_quiet_places',\n",
    "    'family_with_kids',\n",
    "    'fitness_focused',\n",
    "    'frequent_coffee_buyer',\n",
    "    'frequent_pharmacy_visitor',\n",
    "    'frequent_returns',\n",
    "    'frequent_subscriber',\n",
    "    'frequent_takeout_customer',\n",
    "    'fuel_cost_sensitive',\n",
    "    'gift_card_giver',\n",
    "    'goes_solo',\n",
    "    'grocery_delivery_user',\n",
    "    'heavy_phone_user',\n",
    "    'high_end_grocery_buyer',\n",
    "    'history_buff',\n",
    "    'holiday_sale_shopper',\n",
    "    'home_cook',\n",
    "    'home_improvement_spender',\n",
    "    'impulse_buyer',\n",
    "    'international_tourist',\n",
    "    'last_minute_buyer',\n",
    "    'likes_camping',\n",
    "    'likes_luxury_trips',\n",
    "    'likes_museums',\n",
    "    'local_market_shopper',\n",
    "    'loves_hiking',\n",
    "    'loyalty_program_member',\n",
    "    'mobile_grocery_app_user',\n",
    "    'monthly_budget_adjuster',\n",
    "    'mountain_explorer',\n",
    "    'multichannel_shopper',\n",
    "    'nature_lover',\n",
    "    'night_owl',\n",
    "    'organic_food_buyer',\n",
    "    'owns_electric_vehicle',\n",
    "    'pays_with_mobile_apps',\n",
    "    'pet_friendly_traveler',\n",
    "    'photography_enthusiast',\n",
    "    'plans_trips_in_advance',\n",
    "    'posts_on_instagram',\n",
    "    'prefers_chain_stores',\n",
    "    'prefers_contactless',\n",
    "    'prefers_delivery_over_pickup',\n",
    "    'prefers_gift_experiences',\n",
    "    'prefers_local_brands',\n",
    "    'prefers_weekday_travel',\n",
    "    'price_comparer',\n",
    "    'rents_cars_for_travel',\n",
    "    'retired_traveler',\n",
    "    'rewards_points_user',\n",
    "    'road_trip_fan',\n",
    "    'rural_resident',\n",
    "    'shares_travel_on_social_media',\n",
    "    'shops_online_frequently',\n",
    "    'smart_home_user',\n",
    "    'solo_female_traveler',\n",
    "    'spiritual_traveler',\n",
    "    'stationery_and_supplies_shopper',\n",
    "    'streams_music_outdoors',\n",
    "    'subscribes_to_meal_kits',\n",
    "    'subscription_box_user',\n",
    "    'suburban_resident',\n",
    "    'sunset_seeker',\n",
    "    'takes_group_tours',\n",
    "    'takes_lots_of_selfies',\n",
    "    'tech_gadget_buyer',\n",
    "    'tracks_steps_on_phone',\n",
    "    'travel_blogger',\n",
    "    'travels_on_a_budget',\n",
    "    'travels_with_family',\n",
    "    'travels_with_partner',\n",
    "    'urban_resident',\n",
    "    'uses_google_maps',\n",
    "    'uses_smart_watch',\n",
    "    'uses_translation_apps',\n",
    "    'uses_travel_apps',\n",
    "    'walks_to_work',\n",
    "    'waterfall_chaser',\n",
    "    'weekly_grocery_shopper',\n",
    "    'wildlife_spotter',\n",
    "    'young_professional'\n",
    "  )\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea370815",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_binary_classification_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc6b20",
   "metadata": {},
   "source": [
    "# 3. Statistical Frameworks\n",
    "\n",
    "In this section, we revisit A/B testing and introduce **statistical power analysis** ‚Äî a key **pre-experiment** step used to estimate whether an experiment is likely to detect a meaningful effect.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Statistical power** is the probability of correctly detecting a true effect (i.e., rejecting the null hypothesis when the alternative is true).\n",
    "- This is commonly set to **80%**, meaning a **20% chance of Type II error** is acceptable.\n",
    "- Power depends on:\n",
    "  - Expected number of observations (traffic volume)\n",
    "  - Duration of the experiment\n",
    "  - Anticipated conversion uplift\n",
    "\n",
    "While our dataset already contains **post-experiment** data (so power analysis isn‚Äôt strictly necessary), understanding power analysis is critical for **experiment planning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471f6f4",
   "metadata": {},
   "source": [
    "## 3.1 Baseline Analysis\n",
    "\n",
    "To ground this concept in real data, we‚Äôll begin by examining **site traffic and conversion rates** from 2025 ‚Äî a baseline period without experiments. This helps us estimate what‚Äôs feasible in future tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# ------------------------------------------------------\n",
    "# 1. Aggregate visit and sales metrics by calendar month\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  # Truncate visit timestamps to month for aggregation\n",
    "  DATE_TRUNC('MON', visits.visit_timestamp) AS visit_month,\n",
    "\n",
    "  # Count of unique visits per month\n",
    "  COUNT(DISTINCT visits.visit_id) AS visits_count,\n",
    "\n",
    "  # Count of unique sales per month\n",
    "  COUNT(DISTINCT sales.sale_id) AS sales_count,\n",
    "\n",
    "  # Monthly conversion rate = sales / visits\n",
    "  sales_count / visits_count AS conversion_rate\n",
    "\n",
    "FROM visits\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Join sales to determine if each visit resulted in a purchase\n",
    "# ------------------------------------------------------\n",
    "LEFT JOIN sales\n",
    "  ON visits.visit_id = sales.visit_id\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Filter for the calendar year 2025\n",
    "# ------------------------------------------------------\n",
    "WHERE visits.visit_timestamp BETWEEN DATE '2025-01-01' AND DATE '2025-12-31'\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Group by month and order chronologically\n",
    "# ------------------------------------------------------\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcc5e5",
   "metadata": {},
   "source": [
    "## 3.2 Experiment Period Analysis\n",
    "\n",
    "We observe that monthly traffic averages around **16,000‚Äì17,000 visits**, with conversion rates rising from **~6.5% during off-season** to nearly **9% in peak months** (April to August).\n",
    "\n",
    "If we plan to run our experiment during **April to June 2026**, we can use these **higher-performing months** as our baseline for power analysis.\n",
    "\n",
    "By averaging traffic and conversion metrics from this period, we can estimate the volume needed to detect different levels of conversion uplift and ensure our experiment is statistically valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# ------------------------------------------------------\n",
    "# 1. Compute monthly visit and sales metrics for April‚ÄìJune 2025\n",
    "# ------------------------------------------------------\n",
    "WITH cte_base AS (\n",
    "  SELECT\n",
    "    # Truncate visit timestamps to month\n",
    "    DATE_TRUNC('MON', visits.visit_timestamp) AS visit_month,\n",
    "\n",
    "    # Count of unique visits per month\n",
    "    COUNT(DISTINCT visits.visit_id) AS visits_count,\n",
    "\n",
    "    # Count of unique sales per month\n",
    "    COUNT(DISTINCT sales.sale_id) AS sales_count,\n",
    "\n",
    "    # Conversion rate = sales / visits\n",
    "    sales_count / visits_count AS conversion_rate\n",
    "\n",
    "  FROM visits\n",
    "  LEFT JOIN sales ON visits.visit_id = sales.visit_id\n",
    "\n",
    "  # Filter to Q2 2025 only\n",
    "  WHERE visits.visit_timestamp BETWEEN DATE '2025-04-01' AND DATE '2025-06-30'\n",
    "\n",
    "  GROUP BY 1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Compute average metrics across the 3-month baseline period\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  AVG(visits_count) AS visits,\n",
    "  AVG(sales_count) AS sales,\n",
    "  AVG(conversion_rate) AS conversion_rate\n",
    "FROM cte_base;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf525d6",
   "metadata": {},
   "source": [
    "## 3.3 Statistical Power Analysis\n",
    "\n",
    "To understand the impact of test design on statistical power, we simulate a few different rollout scenarios:\n",
    "\n",
    "**Traffic Allocation Scenarios:**\n",
    "- **Scenario 1 ‚Äì Split Test**: 50% target vs 50% control\n",
    "- **Scenario 2 ‚Äì All-In Test**: 90% target vs 10% control\n",
    "- **Scenario 3 ‚Äì Canary Test**: 10% target vs 90% control\n",
    "\n",
    "Each scenario represents a different strategy for deploying AI-powered features to users.\n",
    "\n",
    "**Expected Uplift Assumptions:**\n",
    "- **Conservative**: 5% uplift on a 9% conversion rate\n",
    "- **Moderate**: 10% uplift\n",
    "- **Optimistic**: 15% uplift\n",
    "\n",
    "Using these inputs, we simulate power calculations in SQL to estimate the traffic required for statistical significance.  \n",
    "We then filter results to highlight only those scenarios with **‚â• 50% statistical power**, which corresponds to a **‚â§ 50% chance of Type II error** which is reasonable in practical scenarios - although an 80% level is considered ideal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16ab33",
   "metadata": {},
   "source": [
    "## 3.4 SQL Implementation\n",
    "\n",
    "This query performs a **statistical power analysis** using Monte Carlo simulation to evaluate different A/B test setups. It estimates how likely each test scenario is to detect a meaningful uplift in conversion rates.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4.1 Query Goals\n",
    "\n",
    "Estimate **statistical power** (i.e., the probability of detecting a true effect) for various combinations of:\n",
    "\n",
    "- **Conversion uplifts**: 5%, 10%, 15%\n",
    "- **Target/control traffic splits**: 90/10, 50/50, 10/90\n",
    "- **Experiment durations**: 4, 8, or 12 weeks\n",
    "\n",
    "The analysis assumes a baseline conversion rate of **9%** and **4,000 weekly visits**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4.2 Step-by-Step Breakdown\n",
    "\n",
    "1. **`params` ‚Äì Define test scenarios**  \n",
    "   Generates all combinations of uplift, traffic split, and duration. Calculates the expected number of users in target and control groups.\n",
    "\n",
    "2. **`cte_simulated_target_trials` ‚Äì Simulate 1,000 experiments (target group)**  \n",
    "   For each scenario, simulates whether each user in the **target group** converts using a higher conversion rate (e.g., 9% * 1.10). Each simulation consists of randomly generated outcomes per user.\n",
    "\n",
    "3. **`cte_target_results` ‚Äì Aggregate target simulations**  \n",
    "   Computes the conversion rate in each simulation by averaging user-level outcomes.\n",
    "\n",
    "4. **`cte_simulated_control_trials` ‚Äì Simulate 1,000 experiments (control group)**  \n",
    "   Repeats the same process as the target group but uses the baseline conversion rate (9%).\n",
    "\n",
    "5. **`cte_control_results` ‚Äì Aggregate control simulations**  \n",
    "   Calculates the conversion rate for the control group in each simulation.\n",
    "\n",
    "6. **`cte_joint_trials` ‚Äì Compare target vs control**  \n",
    "   For each simulated trial:\n",
    "   - Computes the **difference** in conversion rates\n",
    "   - Calculates the **pooled standard error**\n",
    "   - Derives the **z-score** to test for a statistically significant difference\n",
    "\n",
    "7. **Final SELECT ‚Äì Estimate statistical power**  \n",
    "   For each scenario:\n",
    "   - Counts how many simulations produced a **z-score > 1.645** (significant at 95% confidence)\n",
    "   - Computes the **estimated power** as the proportion of significant results\n",
    "   - Filters to show only test scenarios with **power ‚â• 50%**\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4.3 SQL Summary\n",
    "This SQL-based simulation helps you understand:\n",
    "- How sample size, test duration, and expected uplift affect your test‚Äôs ability to detect real effects\n",
    "- Which experimental designs are likely to be successful before launching a live A/B test\n",
    "\n",
    "It‚Äôs a practical way to make **data-informed decisions** when planning AI product rollouts or feature experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "# ------------------------------------------------------\n",
    "# 1. Define simulation parameters and label each scenario\n",
    "# ------------------------------------------------------\n",
    "WITH params AS (\n",
    "  SELECT \n",
    "    0.09::DOUBLE AS conversion_rate,                         # Baseline conversion rate\n",
    "    conversion_uplift.unnest AS conversion_uplift,           # Uplift scenarios: 5%, 10%, 15%\n",
    "    target_ratio.unnest AS target_ratio,                     # Target group allocation: 90%, 50%, 10%\n",
    "    duration_weeks.unnest AS duration_weeks,                 # Experiment duration: 4, 8, 12 weeks\n",
    "    1 - target_ratio.unnest AS control_ratio,\n",
    "    4000 AS weekly_volume,\n",
    "    \n",
    "    # Add readable labels based on traffic split\n",
    "    CASE \n",
    "      WHEN target_ratio.unnest = 0.5 THEN 'Split Test (50% Target)'\n",
    "      WHEN target_ratio.unnest = 0.9 THEN 'All-In Test (90% Target)'\n",
    "      WHEN target_ratio.unnest = 0.1 THEN 'Canary Test (10% Target)'\n",
    "      ELSE 'Custom Split'\n",
    "    END AS test_label,\n",
    "    \n",
    "    # Label for uplift magnitude\n",
    "    CASE \n",
    "      WHEN conversion_uplift.unnest = 1.05 THEN 'Conservative Uplift (5%)'\n",
    "      WHEN conversion_uplift.unnest = 1.10 THEN 'Moderate Uplift (10%)'\n",
    "      WHEN conversion_uplift.unnest = 1.15 THEN 'Optimistic Uplift (15%)'\n",
    "      ELSE 'Custom Uplift'\n",
    "    END AS uplift_label,\n",
    "    \n",
    "    ROW_NUMBER() OVER (ORDER BY conversion_uplift, target_ratio, duration_weeks) AS trial_id\n",
    "  FROM UNNEST([1.05, 1.1, 1.15]) AS conversion_uplift\n",
    "  CROSS JOIN UNNEST([0.9, 0.5, 0.1]) AS target_ratio\n",
    "  CROSS JOIN UNNEST([4, 8, 12]) AS duration_weeks\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Simulate 1,000 experiments under the alternative hypothesis (uplift applied)\n",
    "# ------------------------------------------------------\n",
    "cte_simulated_target_trials AS (\n",
    "  SELECT\n",
    "    trial_id,\n",
    "    sample_id.unnest AS sample_id,                           # Simulated experiment ID\n",
    "    conversion_rate * conversion_uplift AS rate,             # Increased conversion rate\n",
    "    weekly_volume,\n",
    "    duration_weeks,\n",
    "    weekly_volume * duration_weeks * target_ratio AS volume, # Target group volume\n",
    "    UNNEST(RANGE(1, ROUND(volume)::BIGINT)) AS draw_id,      # Simulate each user\n",
    "    RANDOM() AS random_variable                              # Random number to simulate outcome\n",
    "  FROM params\n",
    "  CROSS JOIN UNNEST(RANGE(1, 1000)) AS sample_id             # 1000 replications per scenario\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Aggregate target results: compute simulated conversion rates\n",
    "# ------------------------------------------------------\n",
    "cte_target_results AS (\n",
    "  SELECT\n",
    "    trial_id,\n",
    "    sample_id,\n",
    "    rate,\n",
    "    volume,\n",
    "    SUM(CASE WHEN random_variable < rate THEN 1 ELSE 0 END)::FLOAT / volume AS probability\n",
    "  FROM cte_simulated_target_trials\n",
    "  GROUP BY 1,2,3,4\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Simulate control group (null hypothesis) for the same 1,000 trials\n",
    "# ------------------------------------------------------\n",
    "cte_simulated_control_trials AS (\n",
    "  SELECT\n",
    "    trial_id,\n",
    "    sample_id.unnest AS sample_id,\n",
    "    conversion_rate AS rate,                                 # Baseline conversion rate (no uplift)\n",
    "    weekly_volume,\n",
    "    duration_weeks,\n",
    "    weekly_volume * duration_weeks * control_ratio AS volume,\n",
    "    UNNEST(RANGE(1, ROUND(volume)::BIGINT)) AS draw_id,\n",
    "    RANDOM() AS random_variable\n",
    "  FROM params\n",
    "  CROSS JOIN UNNEST(RANGE(1, 1000)) AS sample_id\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Aggregate control results\n",
    "# ------------------------------------------------------\n",
    "cte_control_results AS (\n",
    "  SELECT\n",
    "    trial_id,\n",
    "    sample_id,\n",
    "    rate,\n",
    "    volume,\n",
    "    SUM(CASE WHEN random_variable < rate THEN 1 ELSE 0 END)::FLOAT / volume AS probability\n",
    "  FROM cte_simulated_control_trials\n",
    "  GROUP BY 1,2,3,4\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. Join target and control simulations to compute z-scores\n",
    "# ------------------------------------------------------\n",
    "cte_joint_trials AS (\n",
    "  SELECT\n",
    "    target.trial_id,\n",
    "    target.sample_id,\n",
    "    target.volume AS target_volume,\n",
    "    control.volume AS control_volume,\n",
    "    target.probability,\n",
    "    control.probability,\n",
    "    target.probability - control.probability AS difference,\n",
    "    # Pooled standard error for difference of proportions\n",
    "    SQRT(\n",
    "      (target.rate * (1 - target.rate)) / target.volume +\n",
    "      (control.rate * (1 - control.rate)) / control.volume\n",
    "    ) AS pooled_standard_error,\n",
    "    # Z-score for hypothesis test\n",
    "    difference::FLOAT / pooled_standard_error AS z_score\n",
    "  FROM cte_target_results AS target\n",
    "  INNER JOIN cte_control_results AS control\n",
    "    ON target.trial_id = control.trial_id\n",
    "    AND target.sample_id = control.sample_id\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 7. Final Output: Include test label and power estimate\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  params.trial_id,\n",
    "  params.test_label,\n",
    "  params.uplift_label,\n",
    "  params.conversion_rate,\n",
    "  params.conversion_uplift,\n",
    "  params.target_ratio,\n",
    "  params.duration_weeks,\n",
    "  trials.target_volume,\n",
    "  trials.control_volume,\n",
    "  SUM(CASE WHEN trials.z_score > 1.645 THEN 1 ELSE 0 END) AS null_rejected,\n",
    "  null_rejected::FLOAT / COUNT(trials.*) AS estimated_power\n",
    "FROM cte_joint_trials AS trials\n",
    "INNER JOIN params\n",
    "  ON trials.trial_id = params.trial_id\n",
    "GROUP BY 1,2,3,4,5,6,7,8,9\n",
    "HAVING estimated_power >= 0.5\n",
    "ORDER BY estimated_power DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb0f3e",
   "metadata": {},
   "source": [
    "## 3.5 Experiment Evaluation\n",
    "\n",
    "In this section we will perform our experimental A/B test analysis using a similar framework we've used for the natural language processing challenge.\n",
    "\n",
    "The following SQL pipeline evaluates the effectiveness of a new **product recommendation** feature using visit-level data from Q2 2026.\n",
    "\n",
    "It compares **target group users** (feature enabled) to **control group users** (feature disabled) using standard A/B testing techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5.1 Step-by-Step Breakdown\n",
    "\n",
    "1. **Join Event Data into a Unified Table**  \n",
    "   Combines visit logs with optional feature flags, sales transactions, and product pricing to build a clean analysis-ready dataset.\n",
    "\n",
    "2. **Aggregate Control Group Metrics**  \n",
    "   Calculates key metrics (visit count, sales count, total revenue, and conversion rate) for users who **did not** see the feature.\n",
    "\n",
    "3. **Aggregate Target Group Metrics**  \n",
    "   Performs the same aggregation for users who **did** see the feature.\n",
    "\n",
    "4. **Compare Groups Side-by-Side**  \n",
    "   Cross-joins the control and target summaries to make them available for side-by-side analysis in a single row.\n",
    "\n",
    "5. **Calculate Uplift and Confidence Intervals**  \n",
    "   Computes the **absolute uplift** in conversion rate between target and control groups, and calculates **95% confidence intervals** for each group's conversion rate using standard error formulas.\n",
    "\n",
    "6. **Run Z-Test and Estimate Business Impact**  \n",
    "   - Computes a **z-score** and flags results as \"Significant\" if uplift is statistically valid at the 95% confidence level  \n",
    "   - Estimates **incremental conversions** and **incremental revenue** gained from enabling the feature  \n",
    "   - Projects a **baseline revenue** for comparison by simulating what the target group would have earned with the control group‚Äôs conversion rate\n",
    "\n",
    "7. **Final Output**  \n",
    "   Returns all key experiment metrics ‚Äî conversion rates, uplift, statistical confidence, and estimated business value ‚Äî in a single summary view.\n",
    "\n",
    "---\n",
    "\n",
    "This code helps us understand whether the feature rollout had a **meaningful and statistically significant impact** on user behavior and revenue, using well-established experimental design principles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql experiment_results_df <<\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Join visits with feature flags, sales, and product data\n",
    "# ------------------------------------------------------\n",
    "WITH cte_base AS (\n",
    "  SELECT\n",
    "    visits.visit_timestamp,\n",
    "    visits.visit_id,\n",
    "    visits.user_id,\n",
    "\n",
    "    # Flag whether the feature was active for this visit\n",
    "    CASE WHEN features.feature IS NOT NULL THEN 1 ELSE 0 END AS feature_flag,\n",
    "\n",
    "    # Flag whether a sale occurred during this visit\n",
    "    CASE WHEN sales.sale_id IS NOT NULL THEN 1 ELSE 0 END AS sale_flag,\n",
    "\n",
    "    # Capture sale amount; default to 0 if no product linked\n",
    "    COALESCE(products.price_usd, 0) AS sale_amount\n",
    "\n",
    "  FROM visits\n",
    "  LEFT JOIN features \n",
    "    ON visits.visit_id = features.visit_id\n",
    "  LEFT JOIN sales \n",
    "    ON visits.visit_id = sales.visit_id\n",
    "  LEFT JOIN products \n",
    "    ON sales.product_id = products.product_id\n",
    "  WHERE visits.visit_timestamp BETWEEN DATE '2026-04-01' AND DATE '2026-06-30'\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2. Aggregate control group metrics (feature_flag = 0)\n",
    "# ------------------------------------------------------\n",
    "cte_control AS (\n",
    "  SELECT\n",
    "    COUNT(DISTINCT visit_id) AS control_visit_count,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END) AS control_sales_count,\n",
    "    SUM(sale_amount) AS control_sales_amount,\n",
    "    control_sales_count / control_visit_count AS control_conversion_rate\n",
    "  FROM cte_base\n",
    "  WHERE feature_flag = 0\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3. Aggregate target group metrics (feature_flag = 1)\n",
    "# ------------------------------------------------------\n",
    "cte_target AS (\n",
    "  SELECT\n",
    "    COUNT(DISTINCT visit_id) AS target_visit_count,\n",
    "    COUNT(DISTINCT CASE WHEN sale_flag = 1 THEN visit_id ELSE NULL END) AS target_sales_count,\n",
    "    SUM(sale_amount) AS target_sales_amount,\n",
    "    target_sales_count / target_visit_count AS target_conversion_rate\n",
    "  FROM cte_base\n",
    "  WHERE feature_flag = 1\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. Combine control and target group metrics\n",
    "# ------------------------------------------------------\n",
    "cte_combined AS (\n",
    "  SELECT\n",
    "    control.*,\n",
    "    target.*\n",
    "  FROM cte_target AS target\n",
    "  CROSS JOIN cte_control AS control\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. Calculate uplift and confidence intervals\n",
    "# ------------------------------------------------------\n",
    "cte_stats AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    target_conversion_rate - control_conversion_rate AS absolute_uplift,\n",
    "\n",
    "    # 95% Confidence Interval for target group\n",
    "    target_conversion_rate - 1.96 * SQRT((target_conversion_rate * (1 - target_conversion_rate)) / target_visit_count) AS target_ci_lower,\n",
    "    target_conversion_rate + 1.96 * SQRT((target_conversion_rate * (1 - target_conversion_rate)) / target_visit_count) AS target_ci_upper,\n",
    "\n",
    "    # 95% Confidence Interval for control group\n",
    "    control_conversion_rate - 1.96 * SQRT((control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count) AS control_ci_lower,\n",
    "    control_conversion_rate + 1.96 * SQRT((control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count) AS control_ci_upper,\n",
    "\n",
    "    # Standard error for difference in conversion rates\n",
    "    SQRT(\n",
    "      (target_conversion_rate * (1 - target_conversion_rate)) / target_visit_count +\n",
    "      (control_conversion_rate * (1 - control_conversion_rate)) / control_visit_count\n",
    "    ) AS uplift_se\n",
    "  FROM cte_combined\n",
    "),\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. Compute z-score and estimate impact metrics\n",
    "# ------------------------------------------------------\n",
    "cte_zscore AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    absolute_uplift / uplift_se AS z_score,\n",
    "\n",
    "    # One-tailed 95% significance test\n",
    "    CASE \n",
    "      WHEN absolute_uplift / uplift_se >= 1.645 THEN 'Significant'\n",
    "      ELSE 'Not Significant'\n",
    "    END AS test_result,\n",
    "\n",
    "    # Confidence interval for uplift\n",
    "    absolute_uplift - 1.96 * uplift_se AS uplift_ci_lower,\n",
    "    absolute_uplift + 1.96 * uplift_se AS uplift_ci_upper,\n",
    "\n",
    "    # Estimated number of incremental conversions\n",
    "    target_visit_count * absolute_uplift AS incremental_sales_count,\n",
    "\n",
    "    # Projected baseline revenue if uplift had not occurred\n",
    "    control_conversion_rate * target_visit_count * \n",
    "      (control_sales_amount * 1.0 / NULLIF(control_sales_count, 0)) AS expected_sales_amount_without_uplift,\n",
    "\n",
    "    # Actual incremental sales revenue from test\n",
    "    target_sales_amount - control_sales_amount AS incremental_sales_amount\n",
    "  FROM cte_stats\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 7. Final output: summarized experiment evaluation\n",
    "# ------------------------------------------------------\n",
    "SELECT\n",
    "  # Statistical Test Results\n",
    "  z_score,\n",
    "  test_result,\n",
    "\n",
    "  # Uplift and Confidence Intervals\n",
    "  absolute_uplift,\n",
    "  uplift_ci_lower,\n",
    "  uplift_ci_upper,\n",
    "  incremental_sales_count,\n",
    "  incremental_sales_amount,\n",
    "\n",
    "  # Target Group Metrics\n",
    "  target_visit_count,\n",
    "  target_sales_count,\n",
    "  target_conversion_rate,\n",
    "  target_ci_lower,\n",
    "  target_ci_upper,\n",
    "\n",
    "  # Control Group Metrics\n",
    "  control_visit_count,\n",
    "  control_sales_count,\n",
    "  control_conversion_rate,\n",
    "  control_ci_lower,\n",
    "  control_ci_upper\n",
    "FROM cte_zscore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077dd69",
   "metadata": {},
   "source": [
    "## 3.6 Experimentation Insights\n",
    "\n",
    "Here is an example report we can generate using our calculated metrics from our A/B test framework.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Experiment Results Summary\n",
    "\n",
    "Our A/B test aimed to evaluate whether the new product recommendations feature (enabled in the **target** group) led to improved conversion and revenue performance compared to the control group during Q2 2026.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Statistical Significance\n",
    "\n",
    "- **Z-score**: `32.98`  \n",
    "- **Result**: **Significant** at the 95% confidence level (one-tailed test)\n",
    "\n",
    "This indicates **extremely strong evidence** that the target group outperformed the control group in conversion rate.\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Conversion Performance\n",
    "\n",
    "| Metric                     | Control Group  | Target Group     |\n",
    "|----------------------------|----------------|------------------|\n",
    "| Number of Visits           | 25,425         | 25,424           |\n",
    "| Number of Conversions      | 1,780          | 4,140            |\n",
    "| Conversion Rate            | 7.00%          | 16.28%           |\n",
    "| 95% CI (Conversion Rate)   | [6.69%, 7.31%] | [15.83%, 16.74%] |\n",
    "\n",
    "- **Absolute uplift in conversion rate**: **+9.28%**  \n",
    "- **95% Confidence Interval for uplift**: [8.73%, 9.83%]\n",
    "\n",
    "---\n",
    "\n",
    "#### üí∞ Revenue Impact\n",
    "\n",
    "- **Estimated incremental conversions**: `~2,360` additional sales  \n",
    "- **Incremental sales amount**: **$5,945,660**\n",
    "\n",
    "This represents the **extra revenue** driven by the feature rollout in the target group ‚Äî compared to what would have occurred had they performed like the control group.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå Conclusion\n",
    "\n",
    "The experiment showed a **statistically significant and substantial uplift** in both conversion rate and revenue. These results suggest that enabling the new feature during this peak period had a **highly positive business impact**, and may warrant broader rollout.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
